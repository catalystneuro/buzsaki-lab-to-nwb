{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from pymatreader import read_mat\n",
    "import json \n",
    "\n",
    "project_root = Path(\"/home/heberto/buzaki\")\n",
    "session_path = project_root /  \"fCamk1_200827_sess9\"\n",
    "assert session_path.is_dir()\n",
    "\n",
    "# Dump to a file in the same folder\n",
    "json_directory = Path.cwd() / \"_json_files\"\n",
    "json_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve file paths and sizes\n",
    "session_files_path_list = list(session_path.iterdir())\n",
    "condition = lambda file_path: \".mat\" in file_path.name\n",
    "\n",
    "files = [(file_path.name, f\"size: {file_path.stat().st_size / 1024**2 :,.2f} MiB\") for file_path in session_files_path_list if condition(file_path)]\n",
    "\n",
    "# Sort the list by file size in descending order\n",
    "files.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the maximum width for alignment based on longest file name length\n",
    "max_file_name_length = max(len(name) for name, _ in files)\n",
    "max_size_width = max_file_name_length + len(\"  size: \")\n",
    "\n",
    "# Create new list with formatted strings\n",
    "file_as_string = [f\"{name.ljust(max_file_name_length)} {size}\" for name, size in files]\n",
    "file_as_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_files_path_list = list(session_path.iterdir())\n",
    "condition = lambda file_path : \".mat\" not in file_path.name and file_path.is_file()\n",
    "\n",
    "files = [(file_path.name, f\"size: {file_path.stat().st_size / 1024**2 :,.2f} MiB\") for file_path in session_files_path_list if condition(file_path)]\n",
    "\n",
    "# Sort the list by file size in descending order\n",
    "files.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the maximum width for alignment based on longest file name length\n",
    "max_file_name_length = max(len(name) for name, _ in files)\n",
    "max_size_width = max_file_name_length + len(\"  size: \")\n",
    "\n",
    "# Create new list with formatted strings\n",
    "file_as_string = [f\"{name.ljust(max_file_name_length)} {size}\" for name, size in files]\n",
    "file_as_string\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output:\n",
    "\n",
    "* `analogin.dat`, `8,203.18 MiB` : Probably raw data\n",
    "* `fCamk1_200827_sess9.dat`, `32,812.73 MiB` : TODO\n",
    "* `auxiliary.dat`, `3,076.19 MiB` : TODO\n",
    "* `time.dat`, `2,050.80 MiB` :  TODO\n",
    "* `TrialMapsAndRasters.pptx`, `1.02 MiB` : Collection of rasters, I don't think it matters. \n",
    "* `fCamk1_200827_sess9.lfp`, `1,367.20 MiB` :  LFP\n",
    "* `digitalin.dat`, `1,025.40 MiB` :  TODO\n",
    "* `supply.dat`, `1,025.40 MiB` :  TODO\n",
    "* `pulTime.npy`, `0.32 MiB` : TODO\n",
    "* `fCamk1_200827_sess9_HSE.HSE.evt`, `0.28 MiB` : TODO \n",
    "* `deepSuperficial_classification_fromRipples.png`, `0.14 MiB` : Figure, probably does not matter \n",
    "* `bz_DetectSWR.log`, `0.05 MiB` :  TODO\n",
    "* `bz_DetectSWR_manu.log`, `0.05 MiB` : TODO\n",
    "* `fCamk1_200827_sess9.xml`, `0.04 MiB` : TODO\n",
    "* `fCamk1_200827_sess9.nrs`, `0.00 MiB` : TODO\n",
    "* `4nA pulses.txt`, `0.00 MiB` : TODO\n",
    "* `info.rhd`, `0.00 MiB` : TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folders / Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_files_path_list = list(session_path.iterdir())\n",
    "\n",
    "files = [(file_path.name, f\"size: {file_path.stat().st_size / 1024**2 :,.2f} MiB\") for file_path in session_files_path_list if condition(file_path)]\n",
    "\n",
    "# Sort the list by file size in descending order\n",
    "files.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Determine the maximum width for alignment based on longest file name length\n",
    "max_file_name_length = max(len(name) for name, _ in files)\n",
    "max_size_width = max_file_name_length + len(\"  size: \")\n",
    "\n",
    "# Create new list with formatted strings\n",
    "file_as_string = [f\"{name.ljust(max_file_name_length)} {size}\" for name, size in files]\n",
    "file_as_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore specific files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keys_and_types(dictionary):\n",
    "    output_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            output_dict[key] = build_keys_and_types(value)\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            if value.size > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'shape': str(value.shape)\n",
    "                }\n",
    "            else:\n",
    "                # Print small arrays\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        elif isinstance(value, list):\n",
    "            if len(value) > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'length': len(value)\n",
    "                }\n",
    "            else:\n",
    "                # Print small lists\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        else:\n",
    "            output_dict[key] = {\n",
    "                \"type\": str(type(value)),\n",
    "                \"value\": str(value),\n",
    "            }\n",
    "    return output_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `session.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.session.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sessionInfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.sessionInfo.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Behavior.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.Behavior.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `behavior.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.behavior.cellinfo.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tracking.Behavior.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.Tracking.Behavior.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"tracking\"][\"position\"][\"x\"][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subfolder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Tracking.Behavior.mat` subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / f\"{sub_folder.stem}.Tracking.Behavior.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"tracking\"][\"position\"][\"x\"][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that this in the file above have the same position data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Linearized.Behavior.mat` subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / f\"{sub_folder.stem}.Linearized.Behavior.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"\".join(file_path.suffixes)[1:] + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"behavior\"][\"position\"][\"x\"][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is the same position again here. \n",
    "\n",
    "However, this has the trials as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"behavior\"][\"trials\"][\"startPoint\"][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems the position and not the times as we need. Let's look for other folder to see if we can find the trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera matlab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / \"Basler_acA1280-60gc__21606137__20200827_110730202.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"camera\" + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they just copied the camera frame by frame (!)\n",
    "\n",
    "```\n",
    "\n",
    "{\n",
    "  \"frames\": {\n",
    "    \"r\": {\n",
    "      \"type\": \"<class 'numpy.ndarray'>\",\n",
    "      \"shape\": \"(1024, 204, 53143)\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "How long is the video 53_143 / 30 = 1_771.43 seconds -> 29.52 minutes\n",
    "\n",
    "Taking the sampling_rate of 30 Hz, we can see that the video is 1_771.43 seconds long and the ntransform to minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual maze mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / \"virtualMaze.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = f\"{file_path.stem}\" + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems just like a map of the maze\n",
    "\n",
    "```\n",
    "  \"maze\": {\n",
    "    \"type\": \"<class 'numpy.ndarray'>\",\n",
    "    \"value\": \"[[ 11.09870461 109.25348571]\\n [ 12.12486221   0.85756268]]\"\n",
    "  }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roi tracking .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / \"roiTracking.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = f\"{file_path.stem}\" + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is a just a map with the reigon where they are doing the tracking of the position with the camera:\n",
    "\n",
    "```\n",
    "  \"roiTracking\": {\n",
    "    \"type\": \"<class 'numpy.ndarray'>\",\n",
    "    \"value\": \"[[  31.25 1015.  ]\\n [ 185.75 1019.5 ]\\n [ 184.25    5.5 ]\\n [  17.75    2.5 ]\\n [  31.25 1015.  ]]\"\n",
    "  }\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplifier digital events `.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "file_path = sub_folder / \"amplifier.DigitalIn.events.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = f\"{file_path.stem}\" + \"_sub_folder\" + \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"digitalIn\"].keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "dict_keys(['timestampsOn', 'timestampsOff', 'ints', 'dur', 'intsPeriods'])\n",
    "```\n",
    "I think this is the TTLs that synchronize the events. Come to this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"digitalIn\"][\"timestampsOff\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"digitalIn\"][\"timestampsOff\"][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"digitalIn\"][\"dur\"][0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analog signals in sub-folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following\n",
    "* `auxiliary.dat`\n",
    "* `supply.dat`\n",
    "* `time.dat`\n",
    "* `digitalin.dat`\n",
    "\n",
    "But two xml files:\n",
    "* `fCamk1_200827_sess9.xml` which I guess it the main recorder\n",
    "* `amplifier.xml` which I don't know what it does.\n",
    "\n",
    "The first is on the top directory, the other one is here.\n",
    "\n",
    "Note that in the sub-folder we only have `amplifier.xml` are they different?\n",
    "\n",
    "Let's test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_path =  session_path /  \"fCamk1_200827_sess9.xml\"\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "tree = ElementTree.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "acq = root.find('acquisitionSystem')\n",
    "nbits = int(acq.find('nBits').text)\n",
    "num_channels = int(acq.find('nChannels').text)\n",
    "sampling_rate = float(acq.find('samplingRate').text)\n",
    "voltage_range = float(acq.find('voltageRange').text)\n",
    "# offset = int(acq.find('offset').text)\n",
    "amplification = float(acq.find('amplification').text)\n",
    "\n",
    "print(f\"{num_channels=}, {sampling_rate=}, {voltage_range=}, {amplification=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "\n",
    "xml_file_path =  sub_folder /  \"amplifier.xml\"\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "tree = ElementTree.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "acq = root.find('acquisitionSystem')\n",
    "nbits = int(acq.find('nBits').text)\n",
    "num_channels = int(acq.find('nChannels').text)\n",
    "sampling_rate = float(acq.find('samplingRate').text)\n",
    "voltage_range = float(acq.find('voltageRange').text)\n",
    "# offset = int(acq.find('offset').text)\n",
    "amplification = float(acq.find('amplification').text)\n",
    "\n",
    "print(f\"{num_channels=}, {sampling_rate=}, {voltage_range=}, {amplification=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "The top level:\n",
    "```\n",
    "num_channels=32, sampling_rate=30000.0, voltage_range=20.0, amplification=1000.0\n",
    "```\n",
    "\n",
    "The amplifier:\n",
    "```\n",
    "num_channels=32, sampling_rate=30000.0, voltage_range=20.0, amplification=1000.0\n",
    "```\n",
    "\n",
    "They seem similar. I wonder what we have two but we have a lot of redundancy in the conversion data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = [\"digitalin.dat\", \"time.dat\", \"supply.dat\", \"auxiliary.dat\"]\n",
    "\n",
    "\n",
    "from spikeinterface.extractors.neoextractors import NeuroScopeRecordingExtractor\n",
    "\n",
    "sub_folder = session_path / f\"{session_path.stem.rsplit('_', 1)[0]}_110712\"\n",
    "assert sub_folder.is_dir()\n",
    "\n",
    "for name in signal_names:\n",
    "    file_path = sub_folder / f\"{name}\" \n",
    "    assert file_path.is_file(), file_path\n",
    "\n",
    "    sig_dtype = 'int16' if nbits <= 16 else 'int32'\n",
    "    data = np.memmap(file_path, dtype=sig_dtype, mode='r', offset=0).reshape(-1, num_channels)\n",
    "    time = data.shape[0] / sampling_rate\n",
    "    num_samples = data.shape[0]\n",
    "\n",
    "    print(\"---------------\")\n",
    "    print(f\"{file_path.name=} \\n\")\n",
    "    print(f\"num_samples: {num_samples:,}, time: {time:.2f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "---------------\n",
    "file_path.name='digitalin.dat' \n",
    "\n",
    "num_samples: 1,684,470, time: 56.15 seconds\n",
    "---------------\n",
    "file_path.name='time.dat' \n",
    "\n",
    "num_samples: 3,368,940, time: 112.30 seconds\n",
    "---------------\n",
    "file_path.name='supply.dat' \n",
    "\n",
    "num_samples: 1,684,470, time: 56.15 seconds\n",
    "---------------\n",
    "file_path.name='auxiliary.dat' \n",
    "\n",
    "num_samples: 5,053,410, time: 168.45 seconds\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / \"chanMap.mat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "mat_file = read_mat(file_path) \n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "json_name = \"chanMap\"+ \".json\"\n",
    "with open(json_directory / json_name, 'w') as f:\n",
    "    f.write(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected = mat_file[\"connected\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"chanMap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"chanMap0ind\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analog signals in top folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw signal and LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.dat\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "from spikeinterface.extractors.neoextractors import NeuroScopeRecordingExtractor\n",
    "\n",
    "recording = NeuroScopeRecordingExtractor(file_path=file_path)\n",
    "recording"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```\n",
    "NeuroScopeRecordingExtractor: 32 channels - 30.0kHz - 1 segments - 537,603,840 samples \n",
    "                              17,920.13s (4.98 hours) - int16 dtype - 32.04 GiB\n",
    "  file_path: /home/heberto/buzaki/fCamk1_200827_sess9/fCamk1_200827_sess9.dat\n",
    "```\n",
    "We see this is around 5 hours of recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = session_path / f\"{session_path.stem}.lfp\"\n",
    "assert file_path.is_file(), file_path\n",
    "\n",
    "from spikeinterface.extractors.neoextractors import NeuroScopeRecordingExtractor\n",
    "\n",
    "recording = NeuroScopeRecordingExtractor(file_path=file_path)\n",
    "recording"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the sampling rate is wrong. Let's test with the current value from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 1.25 * 10**3 # 1.25 kHz\n",
    "duration_seconds = recording.get_num_frames() / sampling_rate\n",
    "duration_minutes = duration_seconds / 60.0\n",
    "duration_hours = duration_minutes / 60.0\n",
    "duration_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is five hours. This should be corrected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other signals in top folder\n",
    "We have the following\n",
    "* `analogin.dat`\n",
    "* `auxiliary.dat`\n",
    "* `supply.dat`\n",
    "* `time.dat`\n",
    "* `digitalin.dat`\n",
    "\n",
    "\n",
    "Let's check the XML first from where their metadata comes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file_path =  session_path /  \"fCamk1_200827_sess9.xml\"\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "tree = ElementTree.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "acq = root.find('acquisitionSystem')\n",
    "nbits = int(acq.find('nBits').text)\n",
    "num_channels = int(acq.find('nChannels').text)\n",
    "sampling_rate = float(acq.find('samplingRate').text)\n",
    "voltage_range = float(acq.find('voltageRange').text)\n",
    "# offset = int(acq.find('offset').text)\n",
    "amplification = float(acq.find('amplification').text)\n",
    "\n",
    "print(f\"{num_channels=}, {sampling_rate=}, {voltage_range=}, {amplification=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = [\"digitalin.dat\", \"time.dat\", \"supply.dat\", \"auxiliary.dat\", \"analogin.dat\"]\n",
    "\n",
    "\n",
    "for name in signal_names:\n",
    "    file_path = session_path / f\"{name}\" \n",
    "    assert file_path.is_file(), file_path\n",
    "\n",
    "    sig_dtype = 'int16' if nbits <= 16 else 'int32'\n",
    "    data = np.memmap(file_path, dtype=sig_dtype, mode='r', offset=0).reshape(-1, num_channels)\n",
    "    time = data.shape[0] / sampling_rate\n",
    "    num_samples = data.shape[0]\n",
    "\n",
    "    print(\"---------------\")\n",
    "    print(f\"{file_path.name=} \\n\")\n",
    "    print(f\"num_samples: {num_samples:,}, time: {time / 60.0:.2f} minutes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "They seem too short:\n",
    "```\n",
    "---------------\n",
    "file_path.name='digitalin.dat' \n",
    "\n",
    "num_samples: 16,800,120, time: 9.33 minutes\n",
    "---------------\n",
    "file_path.name='time.dat' \n",
    "\n",
    "num_samples: 33,600,240, time: 18.67 minutes\n",
    "---------------\n",
    "file_path.name='supply.dat' \n",
    "\n",
    "num_samples: 16,800,120, time: 9.33 minutes\n",
    "---------------\n",
    "file_path.name='auxiliary.dat' \n",
    "\n",
    "num_samples: 50,400,360, time: 28.00 minutes\n",
    "---------------\n",
    "file_path.name='analogin.dat' \n",
    "\n",
    "num_samples: 134,400,960, time: 74.67 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroconv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
