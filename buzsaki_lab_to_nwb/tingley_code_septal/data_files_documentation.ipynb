{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f304f4e-6fc3-4168-b759-2261726e7ec8",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "The purpose of this notebook is to describe the files available in the data repository and their structure. \n",
    "\n",
    "For version control purposes this file should be commited without output and only run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4115eb-560c-4ad4-bc31-45556c592113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6855715-5b8a-494c-ac27-0d063d03a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matlab_file(file_path):\n",
    "    file_path = str(file_path)\n",
    "\n",
    "    try:\n",
    "        mat_file = loadmat_mat4py(str(file_path))\n",
    "        mat_file['read'] = 'mat4py'\n",
    "    except:\n",
    "        try:\n",
    "            mat_file = loadmat_mat73(file_path)\n",
    "            mat_file['read'] = 'mat73'\n",
    "        except:\n",
    "            mat_file = loadmat_scipy(file_path)\n",
    "            mat_file['read'] = 'scipy'\n",
    "    return mat_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f7cbc-80b6-4628-84c1-bd608a01d2e3",
   "metadata": {},
   "source": [
    "# Data directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb81b91-f606-452e-b9b7-09b28c7bbc87",
   "metadata": {},
   "source": [
    "First let's just gather all the sesions in a list. This is the list that we use in the converter script to iterate througout all the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc629d6-2fcd-4f18-8d8f-31dca9bb9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('/shared/catalystneuro/Buzsaki/TingleyD')  # Change this with the right location\n",
    "\n",
    "subject_list = ['DT2', 'DT5', 'DT7', 'DT8',  'D79']\n",
    "\n",
    "session_list = [\n",
    "    session\n",
    "    for subject in data_path.iterdir()\n",
    "    if subject.is_dir() and subject.name in subject_list\n",
    "    for session in subject.iterdir()\n",
    "    if session.is_dir() and 'test' not in session.name  # we remove some test sessions\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946738c4-f490-431f-8cf2-a31243efd706",
   "metadata": {},
   "source": [
    "## Subjects \n",
    "First, let's get the different subjects and filter the ones that we decide shoule be included in this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1beb32-822c-4d19-8bbb-0b890a0fc73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_in_study = ['DT2', 'DT5', 'DT7', 'DT8', 'DT9']\n",
    "subject_path_dic = {p.stem:p for p in data_path.iterdir() if p.is_dir() and p.name in subjects_in_study}\n",
    "pprint(subject_path_dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d9848-2677-41ff-8742-bf5f1a819934",
   "metadata": {},
   "source": [
    "The results of this should be all the subjects in this paper\n",
    "['DT2', 'DT5', 'DT7', 'DT8', 'DT9'] and the dictionary values are the paths for each of the subjects folder. \n",
    "\n",
    "Now, it happens that this particular project has a large amount of data. We calculate the folder of each directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb486b5-489f-4c78-b69a-7c2ecac84079",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_sizes_pairs = [(subject, sum([p.stat().st_size for p in path.rglob('*')])) for (subject, path) in subject_path_dic.items()]\n",
    "\n",
    "for subject, size in subject_sizes_pairs:\n",
    "    print(f\"{subject} directory size is {size / (1000 ** 4) :2.2f} TB and {size / (1025 ** 4) :2.2f} TiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd37e30-247c-423b-a9ed-153f20fc2816",
   "metadata": {},
   "source": [
    "The code above should output this:\n",
    "\n",
    "    DT8 directory size is 0.54 TB and 0.49 TiB\n",
    "    DT2 directory size is 4.18 TB and 3.79 TiB\n",
    "    DT9 directory size is 0.83 TB and 0.75 TiB\n",
    "    DT5 directory size is 1.83 TB and 1.66 TiB\n",
    "    DT7 directory size is 1.08 TB and 0.98 TiB\n",
    "\n",
    "The directory associated to subject `DT2` for example is 4.18 terabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507d9af-7155-43e2-a178-9d74140a3499",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6af863-6e18-4ae6-9b81-98c0e6e5e126",
   "metadata": {},
   "source": [
    "Now let's see how the sessions are structured. For this dataset we have that each subject contains multiple files, tentatively belonging to different seessions and some top level files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0e478-df50-40e7-a839-2beed3ad000c",
   "metadata": {},
   "source": [
    "### Session folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f4a6f-8be5-4ba5-a448-87d4fce3a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"DT8\" \n",
    "sessions_path_in_subject = {p.stem:p for p in subject_path_dic[subject].iterdir() if p.is_dir()}\n",
    "session_names_list = list(sessions_path_in_subject.keys())\n",
    "session_names_list.sort()\n",
    "pprint(session_names_list, compact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521ea32-a773-4888-8b92-02780f43773c",
   "metadata": {},
   "source": [
    "The results should be something like:\n",
    "    \n",
    "    ['20170124_0um_0um_170124_151501',\n",
    "     '20170125_0um_0um_merge',\n",
    "     '20170126_0um_0um_merge',\n",
    "     '20170127_0um_0um_170127_121658',\n",
    "     '20170128_0um_72um_merge',\n",
    "     '20170129_0um_72um_170129_130356',\n",
    "     '20170130_0um_144um_170130_143246',\n",
    "     '20170131_72um_432um_merge',\n",
    "     '20170201_72um_432um_merge',\n",
    "     '20170202_72um_576um_170202_092033',\n",
    "     '20170203_72um_648um_170203_105608',\n",
    "     '20170206_72um_1872um_merge',\n",
    "     '20170207_144um_1944um_merge',\n",
    "     '20170208_144um_1944um_merge',\n",
    "     '20170209_144um_1944um_merge',\n",
    "     '20170210_144um_1944um_170210_112254',\n",
    "     '20170211_144um_1944um_merge',\n",
    "     '20170212_144um_1944um_170212_154634',\n",
    "     '20170213_144um_1944um_merge',\n",
    "     '20170214_216um_1944um_170214_102804',\n",
    "     '20170215_216um_1944um_170215_140739',\n",
    "     '20170216_216um_1944um_merge',\n",
    "     '20170217_216um_1944um_merge',\n",
    "     '20170218_216um_1944um_170218_114724',\n",
    "     '20170220_216um_1944um_170220_192456',\n",
    "     '20170221_324um_1944um_170221_101929',\n",
    "     '20170222_324um_2088um_170222_113940',\n",
    "     '20170223_324um_2088um_170223_103741',\n",
    "     '20170224_324um_2088um_170224_101710',\n",
    "     '20170227_324um_2088um_170227_105926',\n",
    "     '20170228_324um_2088um_merge',\n",
    "     '20170229_324um_2088um_merge',\n",
    "     '20170303_324um_2088um_170303_114804',\n",
    "     '20170305_468um_2088um_170305_135233',\n",
    "     '20170306_468um_2088um_170306_113628',\n",
    "     '20170307_540um_2088um_merge',\n",
    "     '20170308_612um_2088um_merge',\n",
    "     '20170309_612um_2088um_170309_093245',\n",
    "     '20170310_612um_2088um_170310_140825',\n",
    "     '20170311_684um_2088um_170311_134350',\n",
    "     '20170316_828um_2088um_170316_093559',\n",
    "     '20170318_828um_2088um_170318_201151',\n",
    "     '20170320_828um_2088um_170320_200023']\n",
    "\n",
    "Which seem to be the different recording sessions with the following format:\n",
    " * `{date}_{x}um_{date_time |merge}`\n",
    " \n",
    " The only subject that does not adapt to this format is subject `DT2`. Here the files have a different format:\n",
    " \n",
    "     ['DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712',\n",
    "     'DT2_rPPC_rCCG_144um_72um_20160207_160207_170623',\n",
    "     'DT2_rPPC_rCCG_2016um_290um_20160214_160214_105540',\n",
    "     'DT2_rPPC_rCCG_218um_218um_20160208_160208_142910',\n",
    "     'DT2_rPPC_rCCG_3036um_362um_20160215_160215_122710',\n",
    "     'DT2_rPPC_rCCG_3108um_434um_20160215_160215_161846',\n",
    "     'DT2_rPPC_rCCG_3324um_1000um_20160217_160217_103121',\n",
    "     'DT2_rPPC_rCCG_3324um_1072um_20160218_160218_133625',\n",
    "     'DT2_rPPC_rCCG_3324um_1144um_20160219_160219_130435',\n",
    "     'DT2_rPPC_rCCG_3324um_650um_20160216_160216_132620',\n",
    "     'DT2_rPPC_rCCG_3324um_794um_20160216_160216_163304',\n",
    "     'DT2_rPPC_rCCG_3396um_1180um_20160220_160220_141839',\n",
    "     'DT2_rPPC_rCCG_3396um_1180um_20160221_160221_135947',\n",
    "     'DT2_rPPC_rCCG_3468um_1216um_20160222_160222_120902',\n",
    "     'DT2_rPPC_rCCG_3468um_1216um_20160223_160223_121321',\n",
    "     'DT2_rPPC_rCCG_3468um_1216um_20160223_160223_173937',\n",
    "     'DT2_rPPC_rCCG_3468um_1216um_20160224_160224_133121',\n",
    "     'DT2_rPPC_rCCG_3468um_1216um_20160225_160225_130701',\n",
    "     'DT2_rPPC_rCCG_3468um_1252um_20160226_160226_124132',\n",
    "     'DT2_rPPC_rCCG_3540um_1288um_20160227_160227_121226',\n",
    "     'DT2_rPPC_rCCG_3540um_1288um_20160228_160228_135001',\n",
    "     'DT2_rPPC_rCCG_3612um_1360um_20160302_160302_134841',\n",
    "     'DT2_rPPC_rCCG_3612um_1360um_20160303_160303_084915',\n",
    "     'DT2_rPPC_rCCG_3612um_1360um_20160304_160304_150220',\n",
    "     'DT2_rPPC_rCCG_3612um_1360um_20160305_160305_141507',\n",
    "     'DT2_rPPC_rCCG_362um_218um_20160209_160209_183610',\n",
    "     'DT2_rPPC_rCCG_650um_218um_20160210_160210_120100',\n",
    "     'DT2_rPPC_rCCG_794um_290um_20160210_160210_225301',\n",
    "     'DT2_rPPC_rCCG_938um_290um_20160212_160212_122801',\n",
    "\n",
    " It seems that the general format is `DT2_rPPC_rCCG_{x}_um_{y}_um_{date}_{time_stamp}_{time_stamp}`. It is not clear yet to me what is the meaning of the second timestamp. Possibility is a notation for an interval and the second one indicates the duration. That is, a (start, duration) interval. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178ed28-37b3-40f0-b752-06138a6b6e4a",
   "metadata": {},
   "source": [
    "## Structure of the session directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb151be-93b5-420d-963d-46d6a1fc5019",
   "metadata": {},
   "source": [
    "Now let's look inside the directory of those directories inside the session path. \n",
    "Let's show some examples taken from the list above for subject DT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e15198-fb47-4066-b6d9-66779e17667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"20170124_0um_0um_170124_151501\" \n",
    "path_of_dirs_in_session = {p.name:p for p in sessions_path_in_subject[session_name].iterdir() if p.is_dir()}\n",
    "directories_in_session = list(path_of_dirs_in_session.keys())\n",
    "directories_in_session.sort(key=lambda x:int(x))\n",
    "pprint(directories_in_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3229993-05f6-4e85-8e24-ad336feae299",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"20170224_324um_2088um_170224_101710\"\n",
    "path_of_dirs_in_session = {p.name:p for p in sessions_path_in_subject[session_name].iterdir() if p.is_dir()}\n",
    "directories_in_session = list(path_of_dirs_in_session.keys())\n",
    "directories_in_session.sort()\n",
    "pprint(directories_in_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7222099-f1f1-4339-a146-3323495fc24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"20170307_540um_2088um_merge\" \n",
    "path_of_dirs_in_session = {p.name:p for p in sessions_path_in_subject[session_name].iterdir() if p.is_dir()}\n",
    "directories_in_session = list(path_of_dirs_in_session.keys())\n",
    "directories_in_session.sort()\n",
    "pprint(directories_in_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d13ac-7f65-4580-9f1f-b7e3c0fb7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name =\"20170308_612um_2088um_merge\"  \n",
    "path_of_dirs_in_session = {p.name:p for p in sessions_path_in_subject[session_name].iterdir() if p.is_dir()}\n",
    "directories_in_session = list(path_of_dirs_in_session.keys())\n",
    "directories_in_session.sort()\n",
    "pprint(directories_in_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6c472-7318-4468-ae9f-534992cac2cc",
   "metadata": {},
   "source": [
    "The output of the last example should be something like this:\n",
    "\n",
    "    ['1',\n",
    "     '2',\n",
    "     '3',\n",
    "     '4',\n",
    "     '5',\n",
    "     '6',\n",
    "     '7',\n",
    "     '8',\n",
    "     '9',\n",
    "     '10',\n",
    "     '20170308_612um_2088um_170308_115428', # these are available on merge sessions\n",
    "     '20170308_612um_2088um_170308_144851', # there are available on merge sessions\n",
    "     'Session 2017-03-08', # not available in every session\n",
    "     'StateScoreFigures'   # not available in every session\n",
    "     ]\n",
    "\n",
    "The general structure seem to be the following. Each of the sessions contains ten folders numerated from 1 to 10. Merge sessions contain further folders wher the specific time-stamps of the merged sessions can be found and finally sometimes there is a folder with the naming \"Session {date}\" \n",
    "\n",
    "Importantly, while subject `DT5` has the same number, 10, of numbered folders the rest of the subject do not. `DT7` has 7 folders whereas `DT9` and `DT2` have 12 and 13 folders respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66863ad9-142f-466e-8db7-d0e09fedb5c2",
   "metadata": {},
   "source": [
    "### Files in each of the sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0ceb3-1629-4b2c-a68e-592ec9f6b0ce",
   "metadata": {},
   "source": [
    "Here a brief description of what type of files are each of the levels. A more extended discussion can be vound in the Data section below\n",
    "\n",
    "In the session directory top-most level the following formats can be found:\n",
    "* evt\n",
    "* clu\n",
    "* fet\n",
    "* pos\n",
    "* res\n",
    "* lfp\n",
    "* spk\n",
    "* nrs\n",
    "* xml\n",
    "* mat\n",
    "\n",
    "This seems to be where the main data for the session is, plus the mat files whereas both processing and behavioral might be.\n",
    "\n",
    "In the directories that are named with numbers the following formats can be found:\n",
    "* kwd \n",
    "* kwik\n",
    "* log\n",
    "* clu\n",
    "* fet\n",
    "* fmask\n",
    "* klg\n",
    "* prm\n",
    "* kvlog\n",
    "\n",
    "This seems the results of `klusta` suite for spike sorting and analysis\n",
    "\n",
    "In merge session when merge directories are available the following files can be found on them:\n",
    "* info.rhd\n",
    "* amplifiers.nrs\n",
    "* amplifiers.xlm\n",
    "\n",
    "Finally, when the directory \"Sesion {date}\" is available, it contains csv files with a time stamp as title and some `.tak` files which seem to be audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb52203-d9ea-49ad-898a-5789e764529e",
   "metadata": {},
   "source": [
    "## Subject top-level files\n",
    "Moreover, we also find some files in the topmost directory. We describe them in more detail in the data section of this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84739b6-1aee-47d8-bb11-f92160e4f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_not_in_session = {p.name:p for p in subject_path_dic[subject].iterdir() if p.is_file()}\n",
    "files_names = list(files_not_in_session.keys())\n",
    "files_names.sort()\n",
    "pprint(files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94bc40-d73b-4415-bcc4-2eb87a821269",
   "metadata": {},
   "source": [
    "The result should be something like this:\n",
    "\n",
    "    ['.DS_Store',\n",
    "     'DT8_current_dataset_parietal_theta.mat',\n",
    "     'behav.mat',\n",
    "     'groupRecordings.m']\n",
    "     \n",
    " They seem to be behavioral files and we will come back to them when we discuss the specific files below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80788b7c-9ae4-43fe-bffb-bbbbd658ddf6",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae14f0c-9305-4084-824e-bb0aca477631",
   "metadata": {},
   "source": [
    "## An overview of the available data\n",
    "We discuss now what formats are present before describing the files in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28aceaa-0960-4669-8394-4ea7e07b80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_data_formats = [\".jpg\", \".png\", \".pdf\", \".svg\", \".fig\", \".py\", \".m\", '.py']\n",
    "\n",
    "format_list = [p.suffixes for p in data_path.rglob(\"*\") if p.is_file()]\n",
    "format_list = list({_ for suffixes in format_list for _ in suffixes}) \n",
    "format_list = [_ for _ in format_list if len(_)==4 and \" \" not in _]  # Only standar three letter formats.\n",
    "format_list = [_ for _ in format_list if not any(map(str.isdigit, _))]  # Remove numbers \n",
    "format_list = [_ for _ in format_list if _ not in not_data_formats] # remove data formts\n",
    "format_list.sort(key=lambda x : x.lower())  # sort by lower case\n",
    "\n",
    "pprint(format_list, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609333a3-f70c-4df2-86c8-e91f3b5ff7cb",
   "metadata": {},
   "source": [
    "The output of this should give us the formats available for this subject and should look something like this:\n",
    "\n",
    "    ['.alg', '.avi', '.cat', '.clu', '.com', '.csv', '.dat', '.EMG', '.evt', '.fet',\n",
    "     '.klg', '.kwd', '.kwx', '.led', '.lfp', '.LFP', '.lnk', '.log', '.LOG', '.low',\n",
    "     '.mat', '.nrs', '.out', '.pos', '.prb', '.prm', '.raw', '.res', '.rhd', '.rLS',\n",
    "     '.rls', '.spk', '.tak', '.txt', '.url', '.WAV', '.xml']\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c852fea-5762-4359-81d6-70dc81074d60",
   "metadata": {},
   "source": [
    "Descripton of the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d18931-e300-4cfa-8b69-3d125bfae362",
   "metadata": {},
   "source": [
    "* `.alg` :\n",
    "* `.avi` : video format.\n",
    "* `.cat` :\n",
    "* `.clu` : usually associated with the neuroscope sorting format\n",
    "* `.com` :\n",
    "* `.csv` : comma separated values.\n",
    "* `.dat` : usually the raw data.\n",
    "* `.EMG` : \n",
    "* `.evt` :\n",
    "* `.fet` :\n",
    "* `.klg` : files related to the klusta spike sorting suit.\n",
    "* `.kwd` : files related to the klusta spike sorting suit.\n",
    "* `.kwx` : files related to the klusta spike sorting suit.\n",
    "* `.led` :\n",
    "* `.lfp` : local field potential data.\n",
    "* `.LFP` : local field potential data\n",
    "* `.lnk` :\n",
    "* `.log` : log files, usually not useful. Sometimes associated with the Phy sorting program.\n",
    "* `.LOG` : log files, usually not useful.\n",
    "* `.low` : \n",
    "* `.mat` : data structures form matlab\n",
    "* `.nrs` :\n",
    "* `.out` :\n",
    "* `.pos` :\n",
    "* `.prb` :\n",
    "* `.prm` : \n",
    "* `.raw` :\n",
    "* `.res` : usually associated with the neuroscope sorting format\n",
    "* `.rhd` :\n",
    "* `.rLS` :\n",
    "* `.rls` :\n",
    "* `.spk` : \n",
    "* `.tak` : \n",
    "* `.txt` : plain text files\n",
    "* `.url` : a url address \n",
    "* `.WAV` : audio format.\n",
    "* `.xml` : xml files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712071a-482d-4fc8-85d6-a45ebf788522",
   "metadata": {},
   "source": [
    "## Dat files - Raw data\n",
    "These files are usually raw data. However, for this project it seems that most files do not have the raw data and only have the lfp.\n",
    "\n",
    "To give us an idea let's get the some files together with their pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500cc14-70f3-4c70-8011-8fc5b495b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_search = \".dat\"\n",
    "path_and_size_pairs = [('/'.join(str(p).split('/')[5:]), f\"{p.stat().st_size/1000**2:2.2f} MB\") for p in data_path.rglob(f\"*{format_to_search}\") if p.is_file()]\n",
    "pprint(path_and_size_pairs[:10], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d76e7-dd3d-4ec4-9efb-4a45f6baf44a",
   "metadata": {},
   "source": [
    "The output should be something like this:\n",
    "\n",
    "    [('DT2/DT2_rPPC_rCCG_3612um_1360um_20160302_160302_134841/DT2_rPPC_rCCG_3612um_1360um_20160302_160302_134841.dat',\n",
    "      '76435.85 MB'),\n",
    "     ('DT2/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712/analogin.dat',\n",
    "      '579.90 MB'),\n",
    "     ('DT2/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712/supply.dat',\n",
    "      '1159.80 MB'),\n",
    "     ('DT2/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712/time.dat',\n",
    "      '1159.80 MB'),\n",
    "     ('DT2/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712.dat',\n",
    "      '74226.95 MB'),\n",
    "     ('DT2/DT2_rPPC_rCCG_1226um_290um_20160211_160213_120712/auxiliary.dat',\n",
    "      '3479.39 MB'),\n",
    "     ('DT2/z_Intruder_test_160304_152951/z_Intruder_test_160304_152951.dat',\n",
    "      '51600.94 MB'),\n",
    "     ('DT2/z_Intruder_test_160304_152951/extras/analogin.dat', '403.13 MB'),\n",
    "     ('DT2/z_Intruder_test_160304_152951/extras/supply.dat', '806.26 MB'),\n",
    "     ('DT2/z_Intruder_test_160304_152951/extras/time.dat', '806.26 MB')]\n",
    "\n",
    "Exploring the dat files above, we see that not all sessions have a .dat file, and that there are some .dat files that do not seem to correspond to sessions. \n",
    "\n",
    "We can create a data frame with complete information of where the raw data is to see if all the subjects have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b3047-0d50-40a7-a88c-91d46c210cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_search = '.dat'\n",
    "names = [session.name for session in session_list]\n",
    "subject = [session.parent.name for session in session_list]\n",
    "path_to_file_list = [list(session.glob(f\"*{format_to_search}\")) for session in session_list]\n",
    "file_names = [[p.name for p in path_list] for path_list in path_to_file_list]\n",
    "matches_list = [len(path) for path in path_to_file_list]\n",
    "#canonic_name = [f\"{session.name}.lfp\"== path_list[0].name if len(path_list) > 0 else np.nan for session, path_list in zip(session_list, path_to_file_list)]\n",
    "canonic_name = [any([f\"{session.name}{format_to_search}\" == p.name for p in path_list]) for session, path_list in zip(session_list, path_to_file_list)]\n",
    "\n",
    "data_dic = {'subject': subject, 'session_name':names, 'matches':matches_list, \n",
    "'matches_name': file_names, 'path_to_files':path_to_file_list, 'canonic_name': canonic_name}\n",
    "df = pd.DataFrame(data_dic)\n",
    "df.sort_values(by=['subject', 'session_name'], ignore_index=True, inplace=True)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d61cb-c4f0-43ce-8847-c0bc3f30b6b1",
   "metadata": {},
   "source": [
    "|     | subject   | session_name                        |   matches | matches_name   | path_to_files   | canonic_name   |\n",
    "|----:|:----------|:------------------------------------|----------:|:---------------|:----------------|:---------------|\n",
    "| 106 | DT8       | 20170201_72um_432um_merge           |         0 | []             | []              | False          |\n",
    "| 117 | DT8       | 20170214_216um_1944um_170214_102804 |         0 | []             | []              | False          |\n",
    "|  33 | DT5       | 20161003_216um_288um_merge          |         0 | []             | []              | False          |\n",
    "|  75 | DT7       | 20170319_288um_0um_170319_123708    |         0 | []             | []              | False          |\n",
    "|  66 | DT5       | 20161105_2196um_2232um_merge        |         0 | []             | []              | False          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495dd0ac-3776-4cf6-9643-2c8675b93331",
   "metadata": {},
   "source": [
    "The dataframe has a row for every session and it looks for matching '.dat' files for that session and if they exist whether they have a name equal to the name of the session (canonical name) which is a standard used in other Buzsaki projects. \n",
    "\n",
    "Most files do not have any matches for `.dat` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857de17-289d-40e3-93a2-1f77aa25797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{100 * (df['matches'] > 0).mean():2.2f} per cent of files contain '.dat' files. {sum((df['matches'] > 0))} matches in total\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f156552-d490-4aa7-96f5-c79c3fefe251",
   "metadata": {},
   "source": [
    "\"21.28 per cent of files contain '.dat' files. 30 matches in total\"\n",
    "\n",
    "Moreover, most of the raw files are concentrated in subject DT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f571f-ad4d-43ac-b7aa-4b3c40e4bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f\"matches > 0\"\n",
    "print(df.query(query_string)['subject'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174c4cc-69e9-464e-b20f-861dfe268b60",
   "metadata": {},
   "source": [
    "|     |   subject |\n",
    "|:----|----------:|\n",
    "| DT2 |        29 |\n",
    "| DT5 |         1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f5c45-a1fd-43b9-82f6-526ccd5e1da1",
   "metadata": {},
   "source": [
    "When `.dat` files are availble they do have a canonic name. The following line should output 1.0 indicating that all the files possess with the structure of:\n",
    "\n",
    "`{session_name}.dat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53dfa6-0ef6-4495-9341-523e8747870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f\"matches > 0\"\n",
    "df.query(query_string)['canonic_name'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663c564-14e7-439f-ae07-b5460ae1c490",
   "metadata": {},
   "source": [
    "# LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe6538-9c3c-4a7a-92db-69bd7cbdb263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c9545-5df9-4db3-b88c-2b98b748c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b35fb816-e7d7-4087-b338-99a07faa9ec5",
   "metadata": {},
   "source": [
    "## Matlab files\n",
    "As the matlab files are the ones usually associated with behavioral data they will be describe first. We will see what files are available on a sesion.\n",
    "Here we use a session with merger to have a picture of the most complicated case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018102f1-cca6-4c35-90c2-aab052a34568",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"20170308_612um_2088um_merge\"  \n",
    "mat_files_paths = {p.name:p for p in sessions_path_in_subject[session_name].rglob('*') if not p.is_dir() and 'mat' in ''.join(p.suffixes)}\n",
    "mat_files_names = list(mat_files_paths.keys())\n",
    "mat_files_names.sort()\n",
    "pprint(mat_files_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c19cd-600d-4240-bcc6-aa8d0cb5b11a",
   "metadata": {},
   "source": [
    "The file above should produce something like:\n",
    "\n",
    "    ['20170308_612um_2088um_merge.behavior.mat',\n",
    "     '20170308_612um_2088um_merge.firingMaps.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.isolationMetrics.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.olypherInfo.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.phaseMaps.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.placeFields.01_pctThresh.mat',\n",
    "     '20170308_612um_2088um_merge.placeFields.05_pctThresh.mat',\n",
    "     '20170308_612um_2088um_merge.placeFields.10_pctThresh.mat',\n",
    "     '20170308_612um_2088um_merge.placeFields.20_pctThresh.mat',\n",
    "     '20170308_612um_2088um_merge.placeFields.40_pctThresh.mat',\n",
    "     '20170308_612um_2088um_merge.positionDecodingGLM_binnedspace_box.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.positionDecodingGLM_binnedspace_box_median.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.positionDecodingMaxCorr_binned_box_median.cellinfo.mat',\n",
    "     '20170308_612um_2088um_merge.sessionInfo.mat',\n",
    "     '20170308_612um_2088um_merge.spikes.cellinfo.mat']\n",
    "     \n",
    "There are similar files in every section that look like {date}_{x}um_{x}.{name}.mat. We now look at all the files available for the subject disambiguation everything but the name at the end to see what is the variety of mat files available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfadc8-cb32-45aa-bf3e-986cd2a321f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_names_simple = [name for name in mat_files_names if len(name.split(\".\")) == 2]\n",
    "mat_files_names_composed = ['.'.join(name.split('.')[1:]) for name in mat_files_names if len(name.split(\".\")) != 2]\n",
    "\n",
    "mat_files_names_formatted = list(set( mat_files_names_composed))\n",
    "mat_files_names_formatted += mat_files_names_simple\n",
    "mat_files_names_formatted.sort()\n",
    "pprint(mat_files_names_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b5d6b-49b8-427d-9f21-db089dc0ea56",
   "metadata": {},
   "source": [
    "This should produce something that looks like this:\n",
    "\n",
    "    ['DT8_current_dataset_parietal_theta.mat',\n",
    "    'PhaseLockingData.cellinfo.mat',\n",
    "    'assembliesCrossRegionData.mat',\n",
    "    'assembliesCrossRegionData_w_theta_sin_cos_coord_vel.mat',\n",
    "    'assembliesCrossRegion_split_w_theta.mat',\n",
    "    'assembliesWithinRegionData_w_theta_sin_cos_coord_vel.mat',\n",
    "    'behav.mat',\n",
    "    'behav_temp.mat',\n",
    "    'behavior.mat',\n",
    "    'firingMaps.cellinfo.mat',\n",
    "    'isolationMetrics.cellinfo.mat',\n",
    "    'ls_RipplePhaseModulation.cellinfo.mat',\n",
    "    'meta.mat',\n",
    "    'noiseCorrs.mat',\n",
    "    'olypherInfo.cellinfo.mat',g. Most likely we will ignore them.\n",
    "    `spk`\n",
    "    'olypherInfo_w_disc.cellinfo.mat',\n",
    "    'phaseMaps.cellinfo.mat',\n",
    "    'placeFields.01_pctThresh.mat',\n",
    "    'placeFields.05_pctThresh.mat',\n",
    "    'placeFields.10_pctThresh.mat',\n",
    "    'placeFields.20_pctThresh.mat',\n",
    "    'placeFields.40_pctThresh.mat',\n",
    "    'positionDecodingGLM.cellinfo.mat',\n",
    "    'positionDecodingGLM_binnedspace_box.cellinfo.mat',\n",
    "    'positionDecodingGLM_binnedspace_box_median.cellinfo.mat',\n",
    "    'positionDecodingGLM_binnedspace_box_nozero.cellinfo.mat',\n",
    "    'positionDecodingGLM_binnedspace_gauss.cellinfo.mat',\n",
    "    'positionDecodingGLM_box.cellinfo.mat',\n",
    "    'positionDecodingGLM_gaussian.cellinfo.mat',\n",
    "    'positionDecodingMaxCorr_binned_box_median.cellinfo.mat',\n",
    "    'referenceFrames.mat',\n",
    "    'sessionInfo.mat',\n",
    "    'spikes.cellinfo.mat']\n",
    "    \n",
    "Which gives us an idea of all the files available. I will describe them in more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27268665-e6fe-441b-a1a8-143d37c354f2",
   "metadata": {},
   "source": [
    "### Description overview (To-do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a6cce-1e4b-4ec2-a899-7d3a8a17ed54",
   "metadata": {},
   "source": [
    "* `DT8_current_dataset_parietal_theta.mat` :\n",
    "* `PhaseLockingData.cellinfo.mat` : \n",
    "* `assembliesCrossRegionData.mat` :  <- Check this\n",
    "* `assembliesCrossRegionData_w_theta_sin_cos_coord_vel.mat` :\n",
    "* `assembliesCrossRegion_split_w_theta.mat` :\n",
    "* `assembliesWithinRegionData_w_theta_sin_cos_coord_vel.mat` :\n",
    "* `behav.mat` : <- Check this\n",
    "* `behav_temp.mat` :\n",
    "* `behavior.mat` :\n",
    "* `firingMaps.cellinfo.mat` :\n",
    "* `isolationMetrics.cellinfo.mat` :\n",
    "* `ls_RipplePhaseModulation.cellinfo.mat` :\n",
    "* `meta.mat` :\n",
    "* `noiseCorrs.mat` : <Ignore this\n",
    "* `olypherInfo.cellinfo.mat` :\n",
    "* `olypherInfo_w_disc.cellinfo.mat` :\n",
    "* `phaseMaps.cellinfo.mat` :\n",
    "* `placeFields.01_pctThresh.mat` :\n",
    "* `placeFields.05_pctThresh.mat` :\n",
    "* `placeFields.10_pctThresh.mat` :\n",
    "* `placeFields.20_pctThresh.mat` :\n",
    "* `placeFields.40_pctThresh.mat` :\n",
    "* `positionDecodingGLM.cellinfo.mat` : Ignore all of these guys -we care about the ACTUAL position not the processing of it. Maybe the od have the x,y z, positions but more likely they are processing parameters for the anlalysis. \n",
    "* `positionDecodingGLM_binnedspace_box.cellinfo.mat` :\n",
    "* `positionDecodingGLM_binnedspace_box_median.cellinfo.mat` :\n",
    "* `positionDecodingGLM_binnedspace_box_nozero.cellinfo.mat` :\n",
    "* `positionDecodingGLM_binnedspace_gauss.cellinfo.mat` :\n",
    "* `positionDecodingGLM_box.cellinfo.mat` :\n",
    "* `positionDecodingGLM_gaussian.cellinfo.mat` :\n",
    "* `positionDecodingMaxCorr_binned_box_median.cellinfo.mat` :\n",
    "* `referenceFrames.mat` :\n",
    "* `sessionInfo.mat` : Initial the cell explorer sorter extrator in the folder paht where this file is. This header was missing from the Yuta file. It will not process the customer \n",
    "* `spikes.cellinfo.mat` :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2defd462-5e65-425c-b212-d8ab58680caf",
   "metadata": {},
   "source": [
    "## CSV files - To do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e4ae3-1e21-4dc6-adc0-6d13020ac74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_search = \".csv\"\n",
    "path_and_size_pairs = [(p.stem, f\"{p.stat().st_size/1000**2:2.2f} MB\") for p in data_path.rglob(f\"*{format_to_search}\") if p.is_file()]\n",
    "pprint(path_and_size_pairs[:10], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf2803-45e2-4a56-a445-184faf63c0d9",
   "metadata": {},
   "source": [
    "The output of the file above should be something like this:\n",
    "\n",
    "    [('Take 2017-02-20 07.26.44 PM', '24.45 MB'),\n",
    "     ('Take 2017-02-01 05.02.41 PM', '39.27 MB'),\n",
    "     ('Take 2017-03-07 03.11.44 PM_batch', '14.27 MB'),\n",
    "     ('Take 2017-03-07 02.29.11 PM_batch', '30.21 MB'),\n",
    "     ('Take 2017-03-07 03.11.44 PM', '13.43 MB'),\n",
    "     ('Take 2017-03-07 02.29.11 PM', '28.50 MB'),\n",
    "     ('Take 2017-02-17 01.48.32 PM', '64.16 MB'),\n",
    "     ('Take 2017-02-08 01.04.32 PM', '25.19 MB'),\n",
    "     ('Take 2017-02-08 01.47.29 PM', '15.13 MB'),\n",
    "     ('Take 2017-02-08 01.42.33 PM', '1.76 MB')]\n",
    "     \n",
    " Those files are very similar among themselves and as discussed previously most of them are located in the folders named \"Session {date}\" \n",
    " \n",
    " Let's open a random path to see what file to see what kind of information they have inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503433d1-77d4-4093-8375-d5114b04931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_search = \".csv\"\n",
    "path_to_csv_list = [p for p in base_path.rglob(f\"*{format_to_search}\") if p.is_file()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844098b3-2bd1-4631-9ccc-384d44c32ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "p = np.random.choice(path_to_csv_list)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8529d088-cbe3-45cb-a982-5f18d5984e55",
   "metadata": {},
   "source": [
    "This should be a path object indicating the complete path to the file:\n",
    "\n",
    "`'/shared/catalystneuro/Buzsaki/TingleyD/DT9/20170522_900um_936um_170522_151132/Session 2017-05-22/Take 2017-05-22 03.37.41 PM_batch.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4cba7-c2c6-4fd7-9ada-756d1e8c4ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(p)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e261f-4d3d-4c56-b3bb-7e19aa83bbee",
   "metadata": {},
   "source": [
    "The initial file does seem to have some columns. We can re-run the following cells to get an idea of how these files look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d20fa-0009-4797-95f8-a6e8aee376d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.choice(path_to_csv_list)\n",
    "print(p)\n",
    "df_csv = pd.read_csv(p)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12bc9a-c3a0-4f64-858c-9cacabb79028",
   "metadata": {},
   "source": [
    "We find that some files seem to be list of positions. The following file is one of those examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea42d6-d52d-4272-a616-b8da5b314f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/home/jovyan/globus_data/TingleyD/DT8/20170217_216um_1944um_merge/Session 2017-02-17/Take 2017-02-17 01.48.32 PM.csv\")\n",
    "df_csv = pd.read_csv(file_path)\n",
    "df_csv.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convert_to_nwb",
   "language": "python",
   "name": "convert_to_nwb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
