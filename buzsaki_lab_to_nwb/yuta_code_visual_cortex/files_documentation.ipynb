{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files documentation\n",
    "The workflow here is to have this notebook to describe in more details the files that are available. For version control\n",
    "purposes this file should be commited without output and only run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import mat73\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Here we load our base path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/home/heberto/globus_data'  # Change this with the right location\n",
    "data_path = Path(data_location)\n",
    "author_path = Path(\"SenzaiY\")\n",
    "base_path = data_path.joinpath(author_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this data sets is organized with one folder per subject. Let's peak inside of  `base_path`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_path_dic = {p.stem:p for p in base_path.iterdir() if p.is_dir()}\n",
    "subject_path_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like ['YMV01', 'YMV02', ...] indicating the different subjects\n",
    "\n",
    "Inside each of the subjects we can find a folder per sesion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'YMV01'\n",
    "sessions_path_dic = {p.stem:p for p in subject_path_dic[subject].iterdir() if p.is_dir()}\n",
    "sessions_path_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput of this should be: `YMV01_170818`. \n",
    "\n",
    "The name of the sessions fits the following pattern `{subject}_{date}`.\n",
    "\n",
    "Let's gather all the available sessions in one dic for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/heberto/globus_data\")\n",
    "author_path = Path(\"SenzaiY\")\n",
    "base_path = data_path.joinpath(author_path)\n",
    "\n",
    "session_list = [\n",
    "    session\n",
    "    for subject in base_path.iterdir()\n",
    "    if subject.is_dir() and \"YMV\" in subject.name\n",
    "    for session in subject.iterdir()\n",
    "]\n",
    "session_path_dic = {session.stem:session for session in session_list if session.is_dir()}\n",
    "session_path_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here should be a combination of session:path for all the sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of the available data\n",
    "Let's find out which data types are available. The files with formats `.jpg`, `.png`, `.fig`, `.pdf`, `.svg` are either photos, vector or documents and we will not be concerned about them so we remove them. We  focus here on the first session on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_data_formats = ['.jpg', '.png', '.pdf', '.svg', '.fig', '.py']\n",
    "\n",
    "subject = 'YMV01'\n",
    "date = '170818'\n",
    "session = f\"{subject}_{date}\"\n",
    "session_path = session_path_dic[session]\n",
    "\n",
    "format_list = list({p.suffix for p in session_path.rglob('*') if not p.is_dir()})\n",
    "format_list.sort()\n",
    "format_list = [p for p in format_list if p not in not_data_formats]\n",
    "pprint(format_list, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like this:\n",
    "\n",
    "    ['', '.1', '.dat', '.eeg', '.json', '.log', '.mat', '.npy', '.nrs',\n",
    "    '.pkl', '.tsv', '.xml']\n",
    "\n",
    "The goal of this document is to explore the data available on the rest of the formats and we will do so the following sections. Meanwhile, for orientation purposes, here is a brief description of the available formats and the files associated with them\n",
    "\n",
    "1. First we have the format '.l' which are actually two formats `.res.1` and `.clu.1`. These are plain files related to the neuroscope sorting format.\n",
    "\n",
    "2. Then we have the typical '.dat' and '.egg' formats that account for the raw data and the local field potential respectively\n",
    "\n",
    "3. The `.json` seem to be associated with hidden files corresponding to the `.phy` format. This is related to spike sorting.\n",
    "\n",
    "4. The `.log` extension is the log file that corresponds to the `phy` program.\n",
    "\n",
    "5. There is a variety of `.mat` files:\n",
    "\n",
    "6. There is a varety of `.npy` files.\n",
    "\n",
    "7. `.nrs`\n",
    "\n",
    "8. `.pkl` pickled file\n",
    "\n",
    "9. `.tsv` tabular separated data.\n",
    "\n",
    "10. `.xml` an xml file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope res and clu\n",
    "These files have a name ofr hte format `{session}.res` and `{session}.clu`. Those should be the keys of the \n",
    "following dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.1'}\n",
    "sorting_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are plain text files and can be opened with pandas as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_file_name = f\"{session}.clu\"\n",
    "res_file_name = f\"{session}.res\"\n",
    "\n",
    "clu_df = pd.read_csv(sorting_files_dic[clu_file_name], header=None, names=['unit'])\n",
    "res_df = pd.read_csv(sorting_files_dic[res_file_name], header=None, names=['times'])\n",
    "res_df.shape, clu_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files should have the same shape. As mentioned those are related to spike sorting. `.clu` contains the units and `.res` the times.\n",
    "We can concatenat them to have the associated ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([clu_df, res_df], axis=1).sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_dic= {p.stem:p for p in session_path.rglob('*') if p.suffix == '.json'}\n",
    "json_files_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files correspond to some meta data of the `phy` software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mat files\n",
    "Let's gather all the mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_dic = {p.stem:p for p in session_path.iterdir() if p.suffix=='.mat'}\n",
    "mat_files_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'cell_metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'cell_metrics'\n",
    "mat_file_path = mat_files_dic[file_name]\n",
    "try:\n",
    "    mat_file = loadmat(mat_file_path)\n",
    "except NotImplementedError:\n",
    "    mat_file = mat73.loadmat(mat_file_path, use_attrdict=True)\n",
    "print(mat_file_path.name, type(mat_file))\n",
    "print(mat_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in mat_files_dic.values():\n",
    "    try:\n",
    "        mat_file = loadmat(file_path)\n",
    "    except NotImplementedError:\n",
    "        mat_file = mat73.loadmat(file_path, use_attrdict=True)\n",
    "    print(file_path.name, type(mat_file))\n",
    "    print(mat_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_files_dic = {p.stem:p for p in session.rglob('*') if p.suffix == '.npy'}\n",
    "numpy_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's the spike_times file to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['spike_times'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['amplitudes'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['channel_map'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['spike_clusters'])\n",
    "np.unique(numpy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['templates'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSV - Tabular separated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf3f1544143e7691e2938dbab5616841bdbe91f209b6ec6b17633b8e86e247c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('buzsaki-lab-to-nwb': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
