{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files documentation\n",
    "The workflow here is to have this notebook to describe in more details the files that are available. For version control\n",
    "purposes this file should be commited without output and only run locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import mat73\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "Here we load our base path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = '/home/heberto/globus_data'  # Change this with the right location\n",
    "data_path = Path(data_location)\n",
    "author_path = Path(\"SenzaiY\")\n",
    "base_path = data_path.joinpath(author_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this data sets is organized with one folder per subject. Let's peak inside of  `base_path`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_path_dic = {p.stem:p for p in base_path.iterdir() if p.is_dir()}\n",
    "subject_path_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like ['YMV01', 'YMV02', ...] indicating the different subjects\n",
    "\n",
    "Inside each of the subjects we can find a folder per sesion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'YMV04'\n",
    "sessions_path_dic = {p.stem:p for p in subject_path_dic[subject].iterdir() if p.is_dir()}\n",
    "sessions_path_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput of this should be: `YMV01_170818`. \n",
    "\n",
    "The name of the sessions fits the following pattern `{subject}_{date}`.\n",
    "\n",
    "Let's gather all the available sessions in one dic for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/home/heberto/globus_data\")\n",
    "author_path = Path(\"SenzaiY\")\n",
    "base_path = data_path.joinpath(author_path)\n",
    "\n",
    "session_list = [\n",
    "    session\n",
    "    for subject in base_path.iterdir()\n",
    "    if subject.is_dir() and \"YMV\" in subject.name\n",
    "    for session in subject.iterdir()\n",
    "]\n",
    "session_path_dic = {session.stem:session for session in session_list if session.is_dir()}\n",
    "session_path_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output here should be a combination of session:path for all the sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An overview of the available data\n",
    "Let's find out which data types are available. The files with formats `.jpg`, `.png`, `.fig`, `.pdf`, `.svg` are either photos, vector or documents and we will not be concerned about them so we remove them. We  focus here on the first session on the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_data_formats = ['.jpg', '.png', '.pdf', '.svg', '.fig', '.py']\n",
    "\n",
    "subject = 'YMV04'\n",
    "date = '170907'\n",
    "session = f\"{subject}_{date}\"\n",
    "session_path = session_path_dic[session]\n",
    "\n",
    "format_list = list({p.suffix for p in session_path.rglob('*') if not p.is_dir()})\n",
    "format_list.sort()\n",
    "format_list = [p for p in format_list if p not in not_data_formats]\n",
    "pprint(format_list, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be something like this:\n",
    "\n",
    "    ['', '.1', '.dat', '.eeg', '.json', '.log', '.mat', '.npy', '.nrs',\n",
    "    '.pkl', '.tsv', '.xml']\n",
    "\n",
    "The goal of this document is to explore the data available on the rest of the formats and we will do so the following sections. Meanwhile, for orientation purposes, here is a brief description of the available formats and the files associated with them\n",
    "\n",
    "1. First we have the format '.l' which are actually two formats `.res.1` and `.clu.1`. These are plain files related to the neuroscope sorting format.\n",
    "\n",
    "2. Then we have the typical '.dat' and '.egg' formats that account for the raw data and the local field potential respectively\n",
    "\n",
    "3. The `.json` seem to be associated with hidden files corresponding to the `.phy` format. This is related to spike sorting.\n",
    "\n",
    "4. The `.log` extension is the log file that corresponds to the `phy` program.\n",
    "\n",
    "5. There is a variety of `.mat` files:\n",
    "\n",
    "6. There is a varety of `.npy` files.\n",
    "\n",
    "7. `.nrs`\n",
    "\n",
    "8. `.pkl` pickled file\n",
    "\n",
    "9. `.tsv` tabular separated data.\n",
    "\n",
    "10. `.xml` an xml file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope res and clu\n",
    "These files have a name ofr hte format `{session}.res` and `{session}.clu`. Those should be the keys of the \n",
    "following dics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.1'}\n",
    "sorting_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are plain text files and can be opened with pandas as a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_file_name = f\"{session}.clu\"\n",
    "res_file_name = f\"{session}.res\"\n",
    "\n",
    "clu_df = pd.read_csv(sorting_files_dic[clu_file_name], header=None, names=['unit'])\n",
    "res_df = pd.read_csv(sorting_files_dic[res_file_name], header=None, names=['times'])\n",
    "res_df.shape, clu_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files should have the same shape. As mentioned those are related to spike sorting. `.clu` contains the units and `.res` the times.\n",
    "We can concatenat them to have the associated ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorting = pd.concat([clu_df, res_df], axis=1)\n",
    "df_sorting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorting['unit'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.xml'}\n",
    "xml_files_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors import NeuroscopeSortingExtractor \n",
    "\n",
    "sorting = NeuroscopeSortingExtractor(resfile_path=sorting_files_dic[res_file_name], \n",
    "clufile_path=sorting_files_dic[clu_file_name], keep_mua_units=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sorting.get_unit_ids(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeextractors import PhySortingExtractor\n",
    "sorting_phy = PhySortingExtractor(folder_path=session_path, exclude_cluster_groups=['noise', 'mua'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sorting_phy.get_unit_ids(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorting_phy.get_unit_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's compare spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorting_phy.get_unit_spike_train(unit_id=15)), len(sorting.get_unit_spike_train(unit_id=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phy_unit_list = sorting_phy.get_unit_ids()  \n",
    "spikes_number_phy = [len(sorting_phy.get_unit_spike_train(unit_id=unit_id)) for unit_id in phy_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuroscope_unit_list = sorting.get_unit_ids()\n",
    "spikes_number_neuro = [len(sorting.get_unit_spike_train(unit_id=unit_id)) for unit_id in neuroscope_unit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_number_phy.sort()\n",
    "spikes_number_neuro.sort()\n",
    "[(x, y) for (x, y) in zip(spikes_number_phy, spikes_number_neuro)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should use the phy by default as we have shown here that they have the same information (removing 'noise' and 'mua'). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_dic= {p.stem:p for p in session_path.rglob('*') if p.suffix == '.json'}\n",
    "json_files_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files correspond to some meta data of the `phy` software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mat files\n",
    "Let's gather all the mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_dic = {p.stem:p for p in session_path.iterdir() if p.suffix=='.mat'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are many files available we will sort them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_files_list = list(mat_files_dic.keys())\n",
    "mat_files_list.sort()\n",
    "pprint(mat_files_list, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the following files:\n",
    "\n",
    "    ['YMV01_170818--InterpDownLFP_params', 'YMV01_170818--InterpUpDownLFP_params',\n",
    "    'YMV01_170818--LFPbasedLayer', 'YMV01_170818-DownUpAlignedLFP-CSD',\n",
    "    'YMV01_170818-MonoSynConvClick', 'YMV01_170818-UnitPhaseMod',\n",
    "    'YMV01_170818.EMGFromLFP.LFP', 'YMV01_170818.SleepScoreLFP.LFP',\n",
    "    'YMV01_170818.SleepScoreMetrics.LFP', 'YMV01_170818.SleepState.states',\n",
    "    'YMV01_170818.SlowWaves.events', 'YMV01_170818.StatePlotMaterials',\n",
    "    'YMV01_170818.cell_metrics.cellinfo', 'YMV01_170818.chanCoords.channelInfo',\n",
    "    'YMV01_170818.eegstates', 'YMV01_170818.mono_res.cellinfo',\n",
    "    'YMV01_170818.noiseLevel.channelInfo', 'YMV01_170818.session',\n",
    "    'YMV01_170818.spikes.cellinfo',\n",
    "    'YMV01_170818.waveform_filter_metrics.cellinfo', 'YMV01_170818_UnitFeature',\n",
    "    'YMV01_170818_meanWaveforms', 'YMV01_170818_wavelet_NREM_8_300Hz',\n",
    "    'YMV01_170818_wavelet_NREM_8_300Hz--Whiten',\n",
    "    'YMV01_170818_wavelet_REM_8_300Hz', 'YMV01_170818_wavelet_REM_8_300Hz--Whiten',\n",
    "    'YMV01_170818_wavelet_WAKE_8_300Hz',\n",
    "    'YMV01_170818_wavelet_WAKE_8_300Hz--Whiten', 'autoclusta_params',\n",
    "    'cell_metrics', 'chanMap', 'depthsort_parameter_1', 'meanWaveforms', 'rez',\n",
    "    'session']\n",
    "\n",
    "\n",
    "Ignore:\n",
    "Anything with param in the name (.e.g depthsort_parameter_1)\n",
    "Anything that has plot in the name (e.g. stateplot_materials)\n",
    "\n",
    "Temporary note here: It is important to note that we add the phy data we should exclude noise and mua (mult-unit activity).\n",
    "\n",
    "\"As a general rule if something can be made by the state data that we use, then it should not be included\" \n",
    "\n",
    "\n",
    "* noiselevel_channel_info . This can be added as an electrodes property.\n",
    "* chanCoords.channel_info . This is duplicated information from chanmap.\n",
    "* SlowWaves.events This can be considered processed data involving up-down intervals. This can be include as process data.\n",
    "* DownUp alignment . Duplication with the LFP. Aligned with specific events. This is for analysis for our concerns this is duplications because we have the base LFP data.\n",
    "* LFPbase layer. It is unclear how to assign this a specific channel. So this is unclear if this is duplicated or analysis.\n",
    "* Sleepscore LFP. Means that specific channels where used for sleep detection. These are the channels that were used for doing some analysis. We can add this as boolean flags to indicate that it was use for . So we will use the channel ID. \n",
    "* Sleepscore metric. Ignore this. Is a part of they made the state classification (REM vs nonRem). But it seems that the information there can be gotten from other pieces of information. It seems that they have the time series for the slow-wave and the theta activity. They assigned some ratio value and then threshold. That is, that's analysis, for the ultimate state that we will add as behavioral data. \n",
    "* EMG for LFP. This we haven seen in previous work but we have not included. Normally EMG that is electromyaography-... is a separated recording usually. As this is used for the state classification we will ignore it.\n",
    "* UnitPhasemod is analysis so we will ignore it. \n",
    "* Eeg states is related EMG and the state classifier and we will ignore it. \n",
    "\n",
    "* Unitfeature contains additional ad-hoc unit properties not covered by `cell_metrics`. \n",
    "\n",
    "\n",
    "We have the three files that correspond to the cell explorer format / interface:\n",
    "* metric_cell_info\n",
    "* mono_res_cellinfo\n",
    "* spikes.cell_info\n",
    "\n",
    "To-do:\n",
    "1) Check if the number of units in the cell-explorer is consitent with either phy or neuroscope. \n",
    "2) \n",
    "\n",
    "* \n",
    "\n",
    "Let's focus on the last files:\n",
    "* `session` : contains behavioral info and general information related to the session such as the experimenter, the species, the strain and timestamps for the creation of the session.\n",
    "* `cell_metrics` : here we find important information concerning the cells as well as some of the session information duplicated. Here we can find information related to the specific cells that were identified in the study such as the number of cell identified,  their brain region, their putative type, etcera. In general these files have an struture equal to the number of cells that were found. That is, structure would be (1, n_cells) where n_cells is the number of the cells identified.\n",
    "* `chanMap` : This seems to be concerned with information of the channels in the electrode. For example we find both the x and y coordinates of each of the channels. The structure of the files here is (1, n_channels) where n_channels is 64 for this setup.\n",
    "* `rez` : contains duplicated information from the `chanMap` concerning the location of the electrodes plus some principal compoennt analysis parameters.\n",
    "\n",
    "Things with wave form and wavelete we ignore.  Because this is information can be computed from the raw data. \n",
    "\n",
    "For sessions that have a a merge file that we will ignore as there is only one `.dat` file. We should investigate the .dat file ensure the files are nan padded. If not, we might need to investigate spliting the electrical series in different start times according to the merge files.\n",
    "\n",
    "`.dat` files, how to open, Neuroscope recording extractor allows you to open dat files and you can look at the traces of a single channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'cell_metrics'\n",
    "mat_file_path = mat_files_dic[file_name]\n",
    "try:\n",
    "    mat_file = loadmat(mat_file_path)\n",
    "except NotImplementedError:\n",
    "    mat_file = mat73.loadmat(mat_file_path, use_attrdict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file['cell_metrics'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file['cell_metrics']['general']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in mat_files_dic.values():\n",
    "    try:\n",
    "        mat_file = loadmat(file_path)\n",
    "    except NotImplementedError:\n",
    "        mat_file = mat73.loadmat(file_path, use_attrdict=True)\n",
    "    print(file_path.name, type(mat_file))\n",
    "    print(mat_file.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.npy'}\n",
    "numpy_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should something like the following files depending on the session\n",
    "\n",
    "    ['templates_ind', 'spike_times', 'templates', 'pc_feature_ind',\n",
    "    'whitening_mat_inv', 'similar_templates', 'spike_clusters', 'template_features', \n",
    "    'spike_templates', 'template_feature_ind', 'amplitudes', 'channel_map',\n",
    "    'pc_features', 'channel_positions', 'whitening_mat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's the spike_times file to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['spike_times'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['amplitudes'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['channel_map'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['spike_clusters'])\n",
    "np.unique(numpy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_file = np.load(numpy_files_dic['templates'])\n",
    "numpy_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.pkl'}\n",
    "pickle_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of those files are in the in the hidden folder for the `phy` software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the files is not working right now. Not priority as it is not clear that we will have to parse  this files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = 'spikes_per_cluster'\n",
    "file_path = pickle_files_dic[file_name]\n",
    "try:\n",
    "    with open(str(file_path), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "except: \n",
    "    print(\"problem oppening this file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSV - Tabular separated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_files_dic = {p.stem:p for p in session_path.rglob('*') if p.suffix == '.tsv'}\n",
    "tsv_files_dic.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only file here is `cluster_group`. Seems related to the spike sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'cluster_group'\n",
    "file_path = tsv_files_dic[file_name]\n",
    "\n",
    "df_cluster_group = pd.read_csv(file_path, sep='\\t')\n",
    "df_cluster_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_group.groupby(['group'])['cluster_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this is is should be something like this:\n",
    "\n",
    "    group\n",
    "    good      53\n",
    "    mua       13\n",
    "    noise    460\n",
    "\n",
    "For the default session in this notebook (the only session for subject YMV01) this seems to indicate that there are 53 good clusters. This corresponds with the cells identified in `cell_metrics.mat`.  My guess right now is that this indicates which of the clusters indicated in `spike_clusters.npy` do correspond to a cell ('good') which ones are noise, etcera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML\n",
    "A file that pairs with the `.dat` and `.eeg` and contains all the header information. This is processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf3f1544143e7691e2938dbab5616841bdbe91f209b6ec6b17633b8e86e247c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('buzsaki-lab-to-nwb': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
