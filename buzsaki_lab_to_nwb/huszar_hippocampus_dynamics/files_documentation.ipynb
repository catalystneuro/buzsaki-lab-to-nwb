{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "\n",
    "# from mat73 import loadmat as loadmat_mat73\n",
    "# from mat4py import loadmat as loadmat_mat4py\n",
    "\n",
    "import h5py\n",
    "from scipy.io import loadmat as loadmat_scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session analysis\n",
    "Let's start with a session to look at the files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and their description. A first take"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peter Petersen, one of the members of the Buzsaki lab, has shared with us the following documents that contain the structure of the cell explorer format:\n",
    "\n",
    "* [new format](https://cellexplorer.org/data-structure/)\n",
    "* [old format](https://github.com/buzsakilab/buzcode/wiki/Data-Formatting-Standards)\n",
    "\n",
    "He referred to the old format as buzcode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_root = Path(\"/home/heberto/buzaki/\") # Heberto\n",
    "project_root = Path(\"/Volumes/neurodata/buzaki/HuszarR\") # Garrett\n",
    "# project_root = Path(\"/shared/catalystneuro/HuszarR\") # DANDI\n",
    "\n",
    "session_path = project_root /  \"optotagCA1/e13/e13_16f1/e13_16f1_210302\"\n",
    "assert session_path.is_dir()\n",
    "\n",
    "# Dump to a file in the same folder\n",
    "json_directory = Path.cwd() / \"_json_files\"\n",
    "json_directory.mkdir(exist_ok=True)\n",
    "\n",
    "# Dump project-wide information into a nested folder\n",
    "project_json_directory = json_directory / \"project\"\n",
    "project_json_directory.mkdir(exist_ok=True)\n",
    "\n",
    "session_files_path_list = list(session_path.iterdir())\n",
    "session_files_path_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as Cody mention, we have seen some experimental data from this lab already and we are familiar with the names.\n",
    "\n",
    "In a [previous conversion](https://github.com/catalystneuro/buzsaki-lab-to-nwb/blob/master/buzsaki_lab_to_nwb/yuta_visual_cortex/files_documentation.ipynb) I found out that the matlab files contain the following information:\n",
    "\n",
    "\n",
    "\n",
    "* `SleepState.states` : This can be considered processed data involving up-down intervals. This can be include as process data.\n",
    "* `chanMap` : This seems to be concerned with information of the channels in the electrode. For example we find both the x and y coordinates of each of the channels. The structure of the files here is (1, n_channels) where n_channels is 64 for this setup.\n",
    "* `session` : Contains behavioral info and general information related to the session such as the experimenter, the species, the strain and timestamps for the creation of the session.\n",
    "\n",
    "As you see, we have the following descriptions missing:\n",
    "* `.ripples.events.mat`: \n",
    "* `Behavior` : \n",
    "\n",
    "Which is something that we would do below.\n",
    "\n",
    "#### Spike sorting\n",
    "The files related to spike sorting were the following in a previous conversion.\n",
    "\n",
    "For the previous conversion those were the files related to **cell explorer format / interface**:\n",
    "* `metric_cell_info`\n",
    "* `mono_res_cellinfo`\n",
    "* `spikes.cell_info`\n",
    "\n",
    "They don't seem equivalent to the ones here. We need to confirm that they are equivalent to the following files in this conversion:\n",
    "* `cell_metrics.cellinfo.mat`\n",
    "* `cell_metrics.cellinfo`\n",
    "* `spikes.cellinfo`\n",
    "\n",
    "This is done below.\n",
    "\n",
    "Apparently, mono_res means Monosynaptic connections.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring some files\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SleepState.states`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see now which matlab file opener works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.SleepState.states.mat\"\n",
    "# Open file_path with loadmat_scipy from scipy\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "# Iterate over the keys and print the type of the values\n",
    "for key in mat_file.keys():\n",
    "    print(key, type(mat_file[key]))\n",
    "\n",
    "print(mat_file['SleepState']['idx']['timestamps'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a recursive structure. Let's print the keys, types and shapes (if numpy array) for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keys_and_types(dictionary):\n",
    "    output_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            output_dict[key] = build_keys_and_types(value)\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            if value.size > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'shape': str(value.shape)\n",
    "                }\n",
    "            else:\n",
    "                # Print small arrays\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        elif isinstance(value, list):\n",
    "            if len(value) > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'length': len(value)\n",
    "                }\n",
    "            else:\n",
    "                # Print small lists\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        else:\n",
    "            output_dict[key] = {\n",
    "                \"type\": str(type(value)),\n",
    "                \"value\": str(value),\n",
    "            }\n",
    "    return output_dict\n",
    "\n",
    "# Define your sleep_state_dict here\n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "with open(json_directory / 'sleep_state_dict.json', 'w') as f:\n",
    "    f.write(json_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a matlab licence, you can also just explore the file there. There are two things to look for, large arrays that might\n",
    "correspond to behavorial data and metadata from the experiment.\n",
    "\n",
    "Because I know the data from this lab I will be looking for the REM state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_state_dict = mat_file[\"SleepState\"]\n",
    "json_dict = json.dumps(build_keys_and_types(sleep_state_dict), indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_state_dict[\"ints\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_state = sleep_state_dict[\"ints\"][\"WAKEstate\"]\n",
    "wake_state  # This is the start time and the end time of the wake state. Probably in frames."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to confirm the units and this can go as a `TimeIntervals` in a processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statenames = sleep_state_dict[\"idx\"][\"statenames\"]\n",
    "timestamps = sleep_state_dict[\"idx\"][\"timestamps\"]\n",
    "states = sleep_state_dict[\"idx\"][\"states\"]\n",
    "\n",
    "states[:5], timestamps[:5], statenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_state = sleep_state_dict[\"ints\"][\"WAKEstate\"]\n",
    "wake_state[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[1803:1810], timestamps[1803:1810]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It apperas to me that the information coincides but the unit of the timestamps are in frames and not in seconds. We need to confirm this.\n",
    "\n",
    "Another possibility is that they are seconds. Let's think this through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps[-10:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last timestamps is 21006 which would indicate that the recording is 21006 seconds long. This is 5.8 hours. We can check on the paper if this is the case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Behavior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_path.parent / \"e13_16f1_210302.Behavior.mat\" \n",
    "file_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'behavior_dict.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly this contains a date in `date` and subject information in `animal`.\n",
    "Surprisingly, this does not seem to contain any large vector as I expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 maze task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = mat_file[\"behavior\"][\"timestamps\"]\n",
    "position = mat_file[\"behavior\"][\"position\"]\n",
    "lin = position[\"lin\"]\n",
    "x = position[\"x\"]\n",
    "y = position[\"y\"]\n",
    "\n",
    "timestamps.shape, lin.shape, x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat_file[\"behavior\"][\"description\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the figure looks like and 8 maze with the dimensions that match the 60 x 60 cm in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = mat_file[\"behavior\"][\"trials\"]\n",
    "trials['choice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num!= num\n",
    "\n",
    "def to_direction(arr):\n",
    "    output = []\n",
    "    for value in arr:\n",
    "\n",
    "        if (isNaN(value)):\n",
    "            output.append('timeout')\n",
    "        \n",
    "        elif (value == 0):\n",
    "            output.append('right')\n",
    "\n",
    "        else:\n",
    "            output.append('left')\n",
    "\n",
    "    return output\n",
    "\n",
    "to_direction(trials['choice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"trial_ints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(lst, condition):\n",
    "    return [i for i, elem in enumerate(lst) if condition(elem)]\n",
    "\n",
    "find_indices(timestamps, lambda e: e > 9972.68103333) #,  9972.68103333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[1824:3000], y[1824:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[7264:9061], y[7264:9061])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes on maps though seems to map the times to something else. I am not sure yet what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"behavior\"][\"maps\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"behavior\"][\"maps\"][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly the timestamps do not start at 0. We need to figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(timestamps, lin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is the trajectory looked as linearly, as that matches with the lenght of the task 205 as described in the paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = mat_file[\"behavior\"][\"trials\"]\n",
    "trials['choice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"trial_ints\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"endDelay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"startPoint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"recorings\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = mat_file[\"behavior\"][\"events\"]\n",
    "events.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"rReward\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ripples.events.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.ripples.events.mat\"\n",
    "file_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'riples.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file['ripples']['timestamps']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large file. We need to look into the paper to see what should be stored from here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `chanMap.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"chanMap.mat\"\n",
    "file_path.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a file for visualization\n",
    "with open(json_directory / 'chanMap.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "colors = {2: \"red\", 1: \"blue\", 3: \"green\", 4: \"yellow\"}\n",
    "colors_for_k_cores = [colors[int(k)] for k in mat_file[\"kcoords\"]]\n",
    "\n",
    "plt.scatter(mat_file[\"xcoords\"][:], mat_file[\"ycoords\"][:], color=colors_for_k_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[\"side\"]\n",
    "colors_for_back = {\"Front\": \"red\", \"Back\": \"blue\"}\n",
    "\n",
    "colors_for_k_cores = [colors_for_back[k] for k in mat_file[\"side\"]]\n",
    "\n",
    "\n",
    "plt.scatter(mat_file[\"xcoords\"][:], mat_file[\"ycoords\"][:], color=colors_for_k_cores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some time series here but no information about the channels as I was expecting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.session.mat\"\n",
    "file_path.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'session.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains useful information like subject, animal and an epoch file.\n",
    "Probably less relevant is information about other sources of data such as spikesorting and some of the analogous channels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikesorting \n",
    "\n",
    "## Testing CellExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.extractors import CellExplorerSortingExtractor\n",
    "\n",
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.spikes.cellinfo.mat\"\n",
    "try:\n",
    "    extractor = CellExplorerSortingExtractor(file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `sessionInfo` for this file so the above throws an assertion. Let's see what each of the files.\n",
    "\n",
    "The `CellExplorerSortingExtractor` uses the `sessionInfo` to extract the sampling frequency. But that might be somehwere else. \n",
    "\n",
    "Importantly, the files should contain the fields `UID` and `times` in a field called `spikes`. Let's see if any of the files contain this information and if it is consistent across them\n",
    "\n",
    "After a second look and some comunication with Petersen it seems that we can use the `spikes.cellinfo` file. We can modify the `CellExplorerSortingExtractor` to use this file instead of the `sessionInfo` file to extract sampling rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual spike files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `spikes.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.spikes.cellinfo.mat\"\n",
    "\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'spikes.cellinfo.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = mat_file[\"spikes\"]\n",
    "spikes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = spikes[\"sr\"]\n",
    "cluster_id = spikes[\"cluID\"]\n",
    "times = spikes[\"times\"]\n",
    "unit_ids = spikes[\"UID\"]\n",
    "\n",
    "sampling_rate, cluster_id.shape, times.shape, unit_ids.shape, unit_ids[:3], cluster_id[:3], times[:3][0][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cell_metrics.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.cell_metrics.cellinfo.mat\"\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Ouput to an external file\n",
    "with open(json_directory / 'cell_metrics.cellinfo.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = mat_file[\"cell_metrics\"][\"spikes\"][\"times\"]\n",
    "unit_ids = mat_file[\"cell_metrics\"][\"UID\"]\n",
    "unit_ids = mat_file[\"cell_metrics\"][\"cluID\"]\n",
    "sampling_rate = None \n",
    "\n",
    "sampling_rate, cluster_id.shape, times.shape, unit_ids.shape, unit_ids[:3], cluster_id[:3], times[:3][0][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mono_ress.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.mono_res.cellinfo.mat\"\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'mono_res.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is somet other analysis concerning `Monosynaptic connections.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cellexplorer with `spikes.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.extractors import CellExplorerSortingExtractor\n",
    "\n",
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.spikes.cellinfo.mat\"\n",
    "\n",
    "try:\n",
    "    extractor = CellExplorerSortingExtractor(file_path, sampling_frequency=30_000.0)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Consistency across Files\n",
    "The following code will loop over all the folders contained in a given directory and track where they occur. This will give us an indication of the file *structure* consistencyâ€”which we can also use to detect whether the data in those files is homogeneous as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def aggregate_file_info(parent_folder, func, aggregator=None):\n",
    "    \n",
    "    if aggregator is None:\n",
    "       aggregator = []\n",
    "\n",
    "\n",
    "    naming_conventions = {}\n",
    "    # for root, dirs, files in tqdm(os.walk(parent_folder)):\n",
    "    #     for file in tqdm(files):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        for file in files:\n",
    "            filename, ext = os.path.splitext(file)\n",
    "            # if filename and filename[0] != '.':\n",
    "            if filename:\n",
    "                if (filename[0] == '.'): \n",
    "                    filename = filename[1:]\n",
    "                split_path = filename.split('.')\n",
    "                naming_convention = '.'.join(split_path[1:]) if len(split_path) > 1 else filename # Remove the '.' character\n",
    "                if naming_convention not in naming_conventions:\n",
    "                    naming_conventions[naming_convention] = [] if (isinstance(aggregator, list)) else {} # Use aggregator type as a base\n",
    "                \n",
    "                info = func(\n",
    "                    file = file,\n",
    "                    naming_convention = naming_convention,\n",
    "                    root = root,\n",
    "                    parent_folder = parent_folder\n",
    "                )\n",
    "\n",
    "                if isinstance(naming_conventions[naming_convention], list):\n",
    "                    naming_conventions[naming_convention].append(info)\n",
    "\n",
    "                else: \n",
    "                    naming_conventions[naming_convention][file] = info\n",
    "                    \n",
    "    return naming_conventions\n",
    "\n",
    "def get_file_name(root, parent_folder, **kwargs):\n",
    "    return os.path.relpath(root, parent_folder)\n",
    "\n",
    "def count_file_naming_conventions(parent_folder):\n",
    "    return aggregate_file_info(parent_folder, get_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to extract the sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_matfile_path = file_path\n",
    "import scipy \n",
    "import hdf5storage\n",
    "\n",
    "try:\n",
    "    spikes_mat = scipy.io.loadmat(file_name=str(spikes_matfile_path))\n",
    "    read_spikes_info_with_scipy = True\n",
    "except NotImplementedError:\n",
    "    spikes_mat = hdf5storage.loadmat(file_name=str(spikes_matfile_path))\n",
    "    read_spikes_info_with_scipy = False\n",
    "cell_info = spikes_mat.get(\"spikes\", np.empty(0))\n",
    "cell_info_fields = cell_info.dtype.names\n",
    "\n",
    "sampling_rate = float(cell_info[\"sr\"][0][0][0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize Project Structure by File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = count_file_naming_conventions(project_root)\n",
    "with open(project_json_directory / 'structure.json', 'w') as f:\n",
    "    f.write(json.dumps(structure, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_entries(arr1, arr2):\n",
    "    for item in arr1:\n",
    "        if item not in arr2:\n",
    "            arr2.append(item)\n",
    "\n",
    "def filter_list(original_list, filter_list):\n",
    "    return [value for value in original_list if value not in filter_list]\n",
    "\n",
    "def check_missing_files(dictionary):\n",
    "    lengths = {}\n",
    "    files = []\n",
    "    for key, value in dictionary.items():\n",
    "        lengths[key] = len(value)\n",
    "        add_unique_entries(value, files)\n",
    "    \n",
    "    missing = dict()\n",
    "    for filetype, length in lengths.items():\n",
    "        if (len(files) != length):\n",
    "            original_list = dictionary[filetype]\n",
    "            missing[filetype] = filter_list(files, original_list)\n",
    "\n",
    "    if len(missing.values()):\n",
    "        print('This dataset has some missing files')\n",
    "        \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = check_missing_files(structure)\n",
    "with open(project_json_directory / 'missing_files.json', 'w') as f:\n",
    "    f.write(json.dumps(missing, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(file, root, **kwargs):\n",
    "    filename, ext = os.path.splitext(file_path)\n",
    "    if (ext == '.mat'):\n",
    "        try:\n",
    "            mat_file = loadmat_scipy(Path(root, file), simplify_cells=True)\n",
    "            return build_keys_and_types(mat_file)\n",
    "        except: \n",
    "            print(f'{file} is not readable by loadmat_scipy')\n",
    "\n",
    "    else:\n",
    "        print(f'Cannot handle {file} file type')\n",
    "        \n",
    "    return {}\n",
    "        \n",
    "def aggregate_data(parent_folder):\n",
    "    return aggregate_file_info(parent_folder, get_file_data, {})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register all the data associated with a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aggregate_data(project_root)\n",
    "with open(project_json_directory / 'data.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the saved data to compare across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(project_json_directory / 'data.json') as user_file:\n",
    "  file_contents = user_file.read()\n",
    "  \n",
    "project_data_json = json.loads(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistency(data, user_check_function, base = ''):\n",
    "    expected_props = set()\n",
    "    inconsistencies = {}\n",
    "\n",
    "    # Loop through objects to discover expected properties\n",
    "    for k, obj in data.items():\n",
    "        props = set(obj.keys())\n",
    "        expected_props = expected_props.union(props)\n",
    "\n",
    "    # Check objects for inconsistencies\n",
    "    registered_nested_properties = set()\n",
    "    for file, obj in data.items():\n",
    "\n",
    "        inconsistent_props = user_check_function(obj, expected_props, base)\n",
    "        if (inconsistent_props and len(inconsistent_props)):\n",
    "            inconsistencies[file] = { f'{base}.{key}': message for key, message in inconsistent_props.items() }\n",
    "            # inconsistencies[file] = [ f'{base}.{key}' if base else key for key, message in inconsistent_props.items() ]\n",
    "\n",
    "        else:\n",
    "            for key in obj.keys():\n",
    "                if isinstance(obj[key], dict):\n",
    "                    registered_nested_properties.add(key)\n",
    "\n",
    "    if (len(registered_nested_properties)):\n",
    "        properties = {}\n",
    "        for key in registered_nested_properties:\n",
    "            nested_property_object = { file: obj[key] for file, obj in data.items() if obj.get(key) }\n",
    "            nested_inconsistencies = check_consistency(nested_property_object, user_check_function, f'{base}.{key}' if base else key)\n",
    "\n",
    "            if (len(nested_inconsistencies)):\n",
    "                properties[key] = nested_inconsistencies\n",
    "\n",
    "        if (len(properties)): \n",
    "            \n",
    "            for key, value in properties.items():\n",
    "                for file, item in value.items():\n",
    "                    if (file not in inconsistencies):\n",
    "                        inconsistencies[file] = {}\n",
    "\n",
    "                    inconsistencies[file].update(item) #f'{base}.{item_string}' if base else item_string)\n",
    "\n",
    "    return inconsistencies\n",
    "\n",
    "\n",
    "\n",
    "def check_missing_props(parent, expected_props, base):\n",
    "        # Check if any properties are missing or have inconsistent values\n",
    "        props = set(parent.keys())\n",
    "        \n",
    "        if props != expected_props:\n",
    "            missing_props = expected_props - props\n",
    "\n",
    "            if missing_props:\n",
    "                return { key: 'Missing' for key in missing_props }\n",
    "\n",
    "\n",
    "# inconsistencies = { key: check_consistency(data, check_missing_props) for key, data in project_data_json.items() }\n",
    "\n",
    "first_key = next(iter(project_data_json))\n",
    "print(project_data_json[first_key].keys())\n",
    "inconsistencies = { key: check_consistency(project_data_json[first_key], check_missing_props) for key, data in project_data_json.items() }\n",
    "\n",
    "with open(project_json_directory / 'data_inconsistencies.json', 'w') as f:\n",
    "    f.write(json.dumps( \n",
    "        { key: data for key, data in inconsistencies.items() if len(data) } , indent=4))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
