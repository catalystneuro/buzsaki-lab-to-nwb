{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "\n",
    "# from mat73 import loadmat as loadmat_mat73\n",
    "# from mat4py import loadmat as loadmat_mat4py\n",
    "\n",
    "import h5py\n",
    "from scipy.io import loadmat as loadmat_scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session analysis\n",
    "Let's start with a session to look at the files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and their description. A first take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.cell_metrics.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.cell_metrics.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.ripples.events.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.ripples.events.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.spikes.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.spikes.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.mono_res.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.mono_res.cellinfo.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.SleepState.states.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.SleepState.states.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.Behavior.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.Behavior.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/e13_16f1_210302.session.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._e13_16f1_210302.session.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/chanMap.mat'),\n",
       " PosixPath('/Volumes/neurodata/buzaki/HuszarR/optotagCA1/e13/e13_16f1/e13_16f1_210302/._chanMap.mat')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = Path(\"/Volumes/neurodata/buzaki/HuszarR\")\n",
    "session_path = Path(project_root, \"optotagCA1/e13/e13_16f1/e13_16f1_210302\")\n",
    "\n",
    "# Dump to a file in the same folder\n",
    "json_directory = Path.cwd() / \"_json_files\"\n",
    "json_directory.mkdir(exist_ok=True)\n",
    "\n",
    "# Dump project-wide information into a nested folder\n",
    "project_json_directory = json_directory / \"project\"\n",
    "project_json_directory.mkdir(exist_ok=True)\n",
    "\n",
    "session_files_path_list = list(session_path.iterdir())\n",
    "session_files_path_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as Cody mention, we have seen some experimental data from this lab already and we are familiar with the names.\n",
    "\n",
    "In a [previous conversion](https://github.com/catalystneuro/buzsaki-lab-to-nwb/blob/master/buzsaki_lab_to_nwb/yuta_visual_cortex/files_documentation.ipynb) I found out that the matlab files contain the following information:\n",
    "\n",
    "\n",
    "\n",
    "* `SleepState.states` : This can be considered processed data involving up-down intervals. This can be include as process data.\n",
    "* `chanMap` : This seems to be concerned with information of the channels in the electrode. For example we find both the x and y coordinates of each of the channels. The structure of the files here is (1, n_channels) where n_channels is 64 for this setup.\n",
    "* `session` : Contains behavioral info and general information related to the session such as the experimenter, the species, the strain and timestamps for the creation of the session.\n",
    "\n",
    "As you see, we have the following descriptions missing:\n",
    "* `.ripples.events.mat`: \n",
    "* `Behavior` : \n",
    "\n",
    "Which is something that we would do below.\n",
    "\n",
    "#### Spike sorting\n",
    "The files related to spike sorting were the following in a previous conversion.\n",
    "\n",
    "For the previous conversion those were the files related to **cell explorer format / interface**:\n",
    "* `metric_cell_info`\n",
    "* `mono_res_cellinfo`\n",
    "* `spikes.cell_info`\n",
    "\n",
    "They don't seem equivalent to the ones here. We need to confirm that they are equivalent to the following files in this conversion:\n",
    "* `cell_metrics.cellinfo.mat`\n",
    "* `cell_metrics.cellinfo`\n",
    "* `spikes.cellinfo`\n",
    "\n",
    "This is done below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring some files\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SleepState.states`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see now which matlab file opener works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__header__ <class 'bytes'>\n",
      "__version__ <class 'str'>\n",
      "__globals__ <class 'list'>\n",
      "SleepState <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.SleepState.states.mat\"\n",
    "# Open file_path with loadmat_scipy from scipy\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "# Iterate over the keys and print the type of the values\n",
    "for key in mat_file.keys():\n",
    "    print(key, type(mat_file[key]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a recursive structure. Let's print the keys, types and shapes (if numpy array) for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_keys_and_types(dictionary):\n",
    "    output_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, dict):\n",
    "            output_dict[key] = build_keys_and_types(value)\n",
    "        elif isinstance(value, np.ndarray):\n",
    "            if value.size > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'shape': str(value.shape)\n",
    "                }\n",
    "            else:\n",
    "                # Print small arrays\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        elif isinstance(value, list):\n",
    "            if len(value) > 10:\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'length': len(value)\n",
    "                }\n",
    "            else:\n",
    "                # Print small lists\n",
    "                output_dict[key] = {\n",
    "                    'type': str(type(value)),\n",
    "                    'value': str(value)\n",
    "                }\n",
    "        else:\n",
    "            output_dict[key] = {\n",
    "                \"type\": str(type(value)),\n",
    "                \"value\": str(value),\n",
    "            }\n",
    "    return output_dict\n",
    "\n",
    "# Define your sleep_state_dict here\n",
    "\n",
    "result = build_keys_and_types(mat_file)\n",
    "json_output = json.dumps(result, indent=2)\n",
    "\n",
    "with open(json_directory / 'sleep_state_dict.json', 'w') as f:\n",
    "    f.write(json_output)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a matlab licence, you can also just explore the file there. There are two things to look for, large arrays that might\n",
    "correspond to behavorial data and metadata from the experiment.\n",
    "\n",
    "Because I know the data from this lab I will be looking for the REM state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_state_dict = mat_file[\"SleepState\"]\n",
    "json_dict = json.dumps(build_keys_and_types(sleep_state_dict), indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WAKEstate': array([[    1,  1806],\n",
       "        [ 2947,  3015],\n",
       "        [ 3314,  3345],\n",
       "        [ 3995,  4025],\n",
       "        [ 4525,  5819],\n",
       "        [ 6221,  6272],\n",
       "        [ 6834,  6876],\n",
       "        [ 7481,  7508],\n",
       "        [ 7614,  7652],\n",
       "        [ 7879,  7897],\n",
       "        [ 8111, 14739],\n",
       "        [15887, 21006]], dtype=uint16),\n",
       " 'NREMstate': array([[ 1807,  2946],\n",
       "        [ 3016,  3241],\n",
       "        [ 3346,  3854],\n",
       "        [ 4026,  4524],\n",
       "        [ 5820,  6220],\n",
       "        [ 6273,  6747],\n",
       "        [ 6877,  7439],\n",
       "        [ 7509,  7593],\n",
       "        [ 7653,  7860],\n",
       "        [ 7898,  8110],\n",
       "        [14740, 15810]], dtype=uint16),\n",
       " 'REMstate': array([[ 3242,  3313],\n",
       "        [ 3855,  3994],\n",
       "        [ 6748,  6833],\n",
       "        [ 7440,  7480],\n",
       "        [ 7594,  7613],\n",
       "        [ 7861,  7878],\n",
       "        [15811, 15886]], dtype=uint16)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_state_dict[\"ints\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,  1806],\n",
       "       [ 2947,  3015],\n",
       "       [ 3314,  3345],\n",
       "       [ 3995,  4025],\n",
       "       [ 4525,  5819],\n",
       "       [ 6221,  6272],\n",
       "       [ 6834,  6876],\n",
       "       [ 7481,  7508],\n",
       "       [ 7614,  7652],\n",
       "       [ 7879,  7897],\n",
       "       [ 8111, 14739],\n",
       "       [15887, 21006]], dtype=uint16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wake_state = sleep_state_dict[\"ints\"][\"WAKEstate\"]\n",
    "wake_state  # This is the start time and the end time of the wake state. Probably in frames."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to confirm the units and this can go as a `TimeIntervals` in a processing module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Behavior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = file_path.parent / \"e13_16f1_210302.Behavior.mat\" \n",
    "file_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'behavior_dict.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly this contains a date in `date` and subject information in `animal`.\n",
    "Surprisingly, this does not seem to contain any large vector as I expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ripples.events.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.ripples.events.mat\"\n",
    "file_path.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'riples.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large file. We need to look into the paper to see what should be stored from here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `chanMap.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"chanMap.mat'\"\n",
    "file_path.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a file for visualization\n",
    "with open(json_directory / 'chanMap.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some time series here but no information about the channels as I was expeting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Session`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = session_files_path_list[0]\n",
    "file_path = file_path.parent / \"e13_16f1_210302.session.mat\"\n",
    "file_path.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'session.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains useful information like subject, animal and an epoch file.\n",
    "Probably less relevant is information about other sources of data such as spikesorting and some of the analogous channels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikesorting \n",
    "\n",
    "## Testing CellExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Exception ignored in: <function tqdm.__del__ at 0x10d84e7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/tqdm/std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x10d84e7a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/tqdm/std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/tqdm/notebook.py\", line 286, in close\n",
      "    self.disp(bar_style='success', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "To use the CellExplorerSortingExtractor install scipy and hdf5storage: \n\n pip install scipy  hdf5storage",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspikeinterface\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextractors\u001b[39;00m \u001b[39mimport\u001b[39;00m CellExplorerSortingExtractor\n\u001b[1;32m      3\u001b[0m file_path \u001b[39m=\u001b[39m session_files_path_list[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mparent \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39me13_16f1_210302.spikes.cellinfo.mat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m extractor \u001b[39m=\u001b[39m CellExplorerSortingExtractor(file_path)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/conversion/lib/python3.11/site-packages/spikeinterface/extractors/cellexplorersortingextractor.py:41\u001b[0m, in \u001b[0;36mCellExplorerSortingExtractor.__init__\u001b[0;34m(self, spikes_matfile_path, session_info_matfile_path, sampling_frequency)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, spikes_matfile_path: PathType,\n\u001b[1;32m     39\u001b[0m              session_info_matfile_path: OptionalPathType\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m              sampling_frequency: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstalled, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstallation_mesg\n\u001b[1;32m     43\u001b[0m     spikes_matfile_path \u001b[39m=\u001b[39m Path(spikes_matfile_path)\n\u001b[1;32m     44\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m         spikes_matfile_path\u001b[39m.\u001b[39mis_file()\n\u001b[1;32m     46\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe spikes_matfile_path (\u001b[39m\u001b[39m{\u001b[39;00mspikes_matfile_path\u001b[39m}\u001b[39;00m\u001b[39m) must exist!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: To use the CellExplorerSortingExtractor install scipy and hdf5storage: \n\n pip install scipy  hdf5storage"
     ]
    }
   ],
   "source": [
    "from spikeinterface.extractors import CellExplorerSortingExtractor\n",
    "\n",
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.spikes.cellinfo.mat\"\n",
    "extractor = CellExplorerSortingExtractor(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No `sessionInfo` for this file so the above throws an assertion. Let's see what each of the files.\n",
    "\n",
    "The `CellExplorerSortingExtractor` uses the `sessionInfo` to extract the sampling frequency. But that might be somehwere else. \n",
    "\n",
    "Importantly, the files should contain the fields `UID` and `times` in a field called `spikes`. Let's see if any of the files contain this information and if it is consistent across them\n",
    "\n",
    "Looking at the files below, they do seem to agree with the basic information. It should be straighforward to role a new sorting extractor for this dataset. Or use NumpySortingExtractor and then add the `sessionInfo` manually. Not sure at this point on what it would be easier. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual spike files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `spikes.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.spikes.cellinfo.mat\"\n",
    "\n",
    "mat_file = loadmat_scipy(file_path, simplify_cells=True)\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'cellinfo.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'ts', 'times', 'cluID', 'maxWaveformCh', 'maxWaveformCh1', 'phy_amp', 'total', 'amplitudes', 'basename', 'numcells', 'UID', 'sr', 'shankID', 'rawWaveform', 'filtWaveform', 'rawWaveform_all', 'rawWaveform_std', 'filtWaveform_all', 'filtWaveform_std', 'timeWaveform', 'timeWaveform_all', 'peakVoltage', 'channels_all', 'peakVoltage_sorted', 'maxWaveform_all', 'peakVoltage_expFitLengthConstant', 'processinginfo'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes = mat_file[\"spikes\"]\n",
    "spikes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,\n",
       " (135,),\n",
       " (135,),\n",
       " (135,),\n",
       " array([1, 2, 3], dtype=uint8),\n",
       " array([ 119, 1162, 1167], dtype=uint16),\n",
       " array([ 68.71136667, 255.81043333, 256.2958    ]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = spikes[\"sr\"]\n",
    "cluster_id = spikes[\"cluID\"]\n",
    "times = spikes[\"times\"]\n",
    "unit_ids = spikes[\"UID\"]\n",
    "\n",
    "sampling_rate, cluster_id.shape, times.shape, unit_ids.shape, unit_ids[:3], cluster_id[:3], times[:3][0][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cell_metrics.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.cell_metrics.cellinfo.mat\"\n",
    "\n",
    "# Ouput to an external file\n",
    "with open(json_directory / 'cell_metrics.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'ts', 'times', 'cluID', 'maxWaveformCh', 'maxWaveformCh1', 'phy_amp', 'total', 'amplitudes', 'basename', 'numcells', 'UID', 'sr', 'shankID', 'rawWaveform', 'filtWaveform', 'rawWaveform_all', 'rawWaveform_std', 'filtWaveform_all', 'filtWaveform_std', 'timeWaveform', 'timeWaveform_all', 'peakVoltage', 'channels_all', 'peakVoltage_sorted', 'maxWaveform_all', 'peakVoltage_expFitLengthConstant', 'processinginfo'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes = mat_file[\"spikes\"]\n",
    "spikes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,\n",
       " (135,),\n",
       " (135,),\n",
       " (135,),\n",
       " array([1, 2, 3], dtype=uint8),\n",
       " array([ 119, 1162, 1167], dtype=uint16),\n",
       " array([ 68.71136667, 255.81043333, 256.2958    ]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = spikes[\"sr\"]\n",
    "cluster_id = spikes[\"cluID\"]\n",
    "times = spikes[\"times\"]\n",
    "unit_ids = spikes[\"UID\"]\n",
    "\n",
    "sampling_rate, cluster_id.shape, times.shape, unit_ids.shape, unit_ids[:3], cluster_id[:3], times[:3][0][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mono_ress.cellinfo.mat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = session_files_path_list[0].parent / \"e13_16f1_210302.mono_res.cellinfo.mat\"\n",
    "\n",
    "# Output to an external file\n",
    "with open(json_directory / 'mono_res.json', 'w') as f:\n",
    "    f.write(json.dumps(build_keys_and_types(mat_file), indent=4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'ts', 'times', 'cluID', 'maxWaveformCh', 'maxWaveformCh1', 'phy_amp', 'total', 'amplitudes', 'basename', 'numcells', 'UID', 'sr', 'shankID', 'rawWaveform', 'filtWaveform', 'rawWaveform_all', 'rawWaveform_std', 'filtWaveform_all', 'filtWaveform_std', 'timeWaveform', 'timeWaveform_all', 'peakVoltage', 'channels_all', 'peakVoltage_sorted', 'maxWaveform_all', 'peakVoltage_expFitLengthConstant', 'processinginfo'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spikes = mat_file[\"spikes\"]\n",
    "spikes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,\n",
       " (135,),\n",
       " (135,),\n",
       " (135,),\n",
       " array([1, 2, 3], dtype=uint8),\n",
       " array([ 119, 1162, 1167], dtype=uint16),\n",
       " array([ 68.71136667, 255.81043333, 256.2958    ]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = spikes[\"sr\"]\n",
    "cluster_id = spikes[\"cluID\"]\n",
    "times = spikes[\"times\"]\n",
    "unit_ids = spikes[\"UID\"]\n",
    "\n",
    "sampling_rate, cluster_id.shape, times.shape, unit_ids.shape, unit_ids[:3], cluster_id[:3], times[:3][0][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Consistency across Files\n",
    "The following code will loop over all the folders contained in a given directory and track where they occur. This will give us an indication of the file *structure* consistencyâ€”which we can also use to detect whether the data in those files is homogeneous as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def aggregate_file_info(parent_folder, func, aggregator=None):\n",
    "    \n",
    "    if aggregator is None:\n",
    "       aggregator = []\n",
    "\n",
    "\n",
    "    naming_conventions = {}\n",
    "    # for root, dirs, files in tqdm(os.walk(parent_folder)):\n",
    "    #     for file in tqdm(files):\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        for file in files:\n",
    "            filename, ext = os.path.splitext(file)\n",
    "            # if filename and filename[0] != '.':\n",
    "            if filename:\n",
    "                if (filename[0] == '.'): \n",
    "                    filename = filename[1:]\n",
    "                split_path = filename.split('.')\n",
    "                naming_convention = '.'.join(split_path[1:]) if len(split_path) > 1 else filename # Remove the '.' character\n",
    "                if naming_convention not in naming_conventions:\n",
    "                    naming_conventions[naming_convention] = [] if (isinstance(aggregator, list)) else {} # Use aggregator type as a base\n",
    "                \n",
    "                info = func(\n",
    "                    file = file,\n",
    "                    naming_convention = naming_convention,\n",
    "                    root = root,\n",
    "                    parent_folder = parent_folder\n",
    "                )\n",
    "\n",
    "                if isinstance(naming_conventions[naming_convention], list):\n",
    "                    naming_conventions[naming_convention].append(info)\n",
    "\n",
    "                else: \n",
    "                    naming_conventions[naming_convention][file] = info\n",
    "                    \n",
    "    return naming_conventions\n",
    "\n",
    "def get_file_name(root, parent_folder, **kwargs):\n",
    "    return os.path.relpath(root, parent_folder)\n",
    "\n",
    "def count_file_naming_conventions(parent_folder):\n",
    "    return aggregate_file_info(parent_folder, get_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize Project Structure by File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = count_file_naming_conventions(project_root)\n",
    "with open(project_json_directory / 'structure.json', 'w') as f:\n",
    "    f.write(json.dumps(structure, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missing File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unique_entries(arr1, arr2):\n",
    "    for item in arr1:\n",
    "        if item not in arr2:\n",
    "            arr2.append(item)\n",
    "\n",
    "def filter_list(original_list, filter_list):\n",
    "    return [value for value in original_list if value not in filter_list]\n",
    "\n",
    "def check_missing_files(dictionary):\n",
    "    lengths = {}\n",
    "    files = []\n",
    "    for key, value in dictionary.items():\n",
    "        lengths[key] = len(value)\n",
    "        add_unique_entries(value, files)\n",
    "    \n",
    "    missing = dict()\n",
    "    for filetype, length in lengths.items():\n",
    "        if (len(files) != length):\n",
    "            original_list = dictionary[filetype]\n",
    "            missing[filetype] = filter_list(files, original_list)\n",
    "\n",
    "    if len(missing.values()):\n",
    "        print('This dataset has some missing files')\n",
    "        \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has some missing files\n"
     ]
    }
   ],
   "source": [
    "missing = check_missing_files(structure)\n",
    "with open(project_json_directory / 'missing_files.json', 'w') as f:\n",
    "    f.write(json.dumps(missing, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_data(file, root, **kwargs):\n",
    "    filename, ext = os.path.splitext(file_path)\n",
    "    if (ext == '.mat'):\n",
    "        try:\n",
    "            mat_file = loadmat_scipy(Path(root, file), simplify_cells=True)\n",
    "            return build_keys_and_types(mat_file)\n",
    "        except: \n",
    "            print(f'{file} is not readable by loadmat_scipy')\n",
    "\n",
    "    else:\n",
    "        print(f'Cannot handle {file} file type')\n",
    "        \n",
    "    return {}\n",
    "        \n",
    "def aggregate_data(parent_folder):\n",
    "    return aggregate_file_info(parent_folder, get_file_data, {})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register all the data associated with a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aggregate_data(project_root)\n",
    "with open(project_json_directory / 'data.json', 'w') as f:\n",
    "    f.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the saved data to compare across sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(project_json_directory / 'data.json') as user_file:\n",
    "  file_contents = user_file.read()\n",
    "  \n",
    "project_data_json = json.loads(file_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['._optotagCA1'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m first_key \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(project_data_json))\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(project_data_json[first_key]\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m---> 61\u001b[0m inconsistencies \u001b[39m=\u001b[39m { key: check_consistency(project_data_json[first_key], check_missing_props) \u001b[39mfor\u001b[39;49;00m key, data \u001b[39min\u001b[39;49;00m project_data_json\u001b[39m.\u001b[39;49mitems() }\n\u001b[1;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(json_directory \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_inconsistencies.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     64\u001b[0m     f\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps( \n\u001b[1;32m     65\u001b[0m         { key: data \u001b[39mfor\u001b[39;00m key, data \u001b[39min\u001b[39;00m inconsistencies\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) } , indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[43], line 61\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m first_key \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(project_data_json))\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(project_data_json[first_key]\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m---> 61\u001b[0m inconsistencies \u001b[39m=\u001b[39m { key: check_consistency(project_data_json[first_key], check_missing_props) \u001b[39mfor\u001b[39;00m key, data \u001b[39min\u001b[39;00m project_data_json\u001b[39m.\u001b[39mitems() }\n\u001b[1;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(json_directory \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_inconsistencies.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     64\u001b[0m     f\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps( \n\u001b[1;32m     65\u001b[0m         { key: data \u001b[39mfor\u001b[39;00m key, data \u001b[39min\u001b[39;00m inconsistencies\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) } , indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[43], line 7\u001b[0m, in \u001b[0;36mcheck_consistency\u001b[0;34m(data, user_check_function, base)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Loop through objects to discover expected properties\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m k, obj \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n\u001b[0;32m----> 7\u001b[0m     props \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(obj\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m      8\u001b[0m     expected_props \u001b[39m=\u001b[39m expected_props\u001b[39m.\u001b[39munion(props)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Check objects for inconsistencies\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "def check_consistency(data, user_check_function, base = ''):\n",
    "    expected_props = set()\n",
    "    inconsistencies = {}\n",
    "\n",
    "    # Loop through objects to discover expected properties\n",
    "    for k, obj in data.items():\n",
    "        props = set(obj.keys())\n",
    "        expected_props = expected_props.union(props)\n",
    "\n",
    "    # Check objects for inconsistencies\n",
    "    registered_nested_properties = set()\n",
    "    for file, obj in data.items():\n",
    "\n",
    "        inconsistent_props = user_check_function(obj, expected_props, base)\n",
    "        if (inconsistent_props and len(inconsistent_props)):\n",
    "            inconsistencies[file] = { f'{base}.{key}': message for key, message in inconsistent_props.items() }\n",
    "            # inconsistencies[file] = [ f'{base}.{key}' if base else key for key, message in inconsistent_props.items() ]\n",
    "\n",
    "        else:\n",
    "            for key in obj.keys():\n",
    "                if isinstance(obj[key], dict):\n",
    "                    registered_nested_properties.add(key)\n",
    "\n",
    "    if (len(registered_nested_properties)):\n",
    "        properties = {}\n",
    "        for key in registered_nested_properties:\n",
    "            nested_property_object = { file: obj[key] for file, obj in data.items() if obj.get(key) }\n",
    "            nested_inconsistencies = check_consistency(nested_property_object, user_check_function, f'{base}.{key}' if base else key)\n",
    "\n",
    "            if (len(nested_inconsistencies)):\n",
    "                properties[key] = nested_inconsistencies\n",
    "\n",
    "        if (len(properties)): \n",
    "            \n",
    "            for key, value in properties.items():\n",
    "                for file, item in value.items():\n",
    "                    if (file not in inconsistencies):\n",
    "                        inconsistencies[file] = {}\n",
    "\n",
    "                    inconsistencies[file].update(item) #f'{base}.{item_string}' if base else item_string)\n",
    "\n",
    "    return inconsistencies\n",
    "\n",
    "\n",
    "\n",
    "def check_missing_props(parent, expected_props, base):\n",
    "        # Check if any properties are missing or have inconsistent values\n",
    "        props = set(parent.keys())\n",
    "        \n",
    "        if props != expected_props:\n",
    "            missing_props = expected_props - props\n",
    "\n",
    "            if missing_props:\n",
    "                return { key: 'Missing' for key in missing_props }\n",
    "\n",
    "\n",
    "# inconsistencies = { key: check_consistency(data, check_missing_props) for key, data in project_data_json.items() }\n",
    "\n",
    "first_key = next(iter(project_data_json))\n",
    "print(project_data_json[first_key].keys())\n",
    "inconsistencies = { key: check_consistency(project_data_json[first_key], check_missing_props) for key, data in project_data_json.items() }\n",
    "\n",
    "with open(project_json_directory / 'data_inconsistencies.json', 'w') as f:\n",
    "    f.write(json.dumps( \n",
    "        { key: data for key, data in inconsistencies.items() if len(data) } , indent=4))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroconv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
